{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6c4cc3",
   "metadata": {},
   "source": [
    "# IIC-3641 GML UC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c409ab",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.10.2\n",
    "- DGL: https://www.dgl.ai/pages/start.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976c1a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f683396",
   "metadata": {},
   "source": [
    "## DGL requiere de un framework de backend. Aquí va con torch sobre cuda (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5284e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ceb47d",
   "metadata": {},
   "source": [
    "## Definiremos una capa GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd3f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain(\"relu\")\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        z2 = torch.cat([edges.src[\"z\"], edges.dst[\"z\"]], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {\"e\": F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {\"z\": edges.src[\"z\"], \"e\": edges.data[\"e\"]}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox[\"e\"], dim=1)\n",
    "        h = torch.sum(alpha * nodes.mailbox[\"z\"], dim=1)\n",
    "        return {\"h\": h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata[\"z\"] = z\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop(\"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d955d8b",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073ef269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge=\"cat\"):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == \"cat\":\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742391a",
   "metadata": {},
   "source": [
    "## Definimos la GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73ee799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cd5bf",
   "metadata": {},
   "source": [
    "## Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6296406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    g = data[0]\n",
    "    train_mask = torch.BoolTensor(g.ndata[\"train_mask\"])\n",
    "    test_mask = torch.BoolTensor(g.ndata[\"test_mask\"])\n",
    "    \n",
    "    return g, g.ndata[\"feat\"], g.ndata[\"label\"], train_mask, test_mask\n",
    "\n",
    "\n",
    "g, features, labels, train_mask, test_mask = load_cora_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2185df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc3203",
   "metadata": {},
   "source": [
    "## Definimos la red y el ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9964ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/marcelo/.local/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.9471 | Test Acc 0.1310 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.9450 | Test Acc 0.1660 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.9430 | Test Acc 0.1940 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.9409 | Test Acc 0.2380 | Time(s) 0.0592\n",
      "Epoch 00004 | Loss 1.9388 | Test Acc 0.2700 | Time(s) 0.0572\n",
      "Epoch 00005 | Loss 1.9368 | Test Acc 0.3100 | Time(s) 0.0572\n",
      "Epoch 00006 | Loss 1.9347 | Test Acc 0.3410 | Time(s) 0.0570\n",
      "Epoch 00007 | Loss 1.9326 | Test Acc 0.3760 | Time(s) 0.0566\n",
      "Epoch 00008 | Loss 1.9305 | Test Acc 0.4200 | Time(s) 0.0563\n",
      "Epoch 00009 | Loss 1.9284 | Test Acc 0.4460 | Time(s) 0.0566\n",
      "Epoch 00010 | Loss 1.9264 | Test Acc 0.4640 | Time(s) 0.0564\n",
      "Epoch 00011 | Loss 1.9243 | Test Acc 0.4780 | Time(s) 0.0562\n",
      "Epoch 00012 | Loss 1.9222 | Test Acc 0.5010 | Time(s) 0.0563\n",
      "Epoch 00013 | Loss 1.9200 | Test Acc 0.5150 | Time(s) 0.0562\n",
      "Epoch 00014 | Loss 1.9179 | Test Acc 0.5340 | Time(s) 0.0561\n",
      "Epoch 00015 | Loss 1.9158 | Test Acc 0.5540 | Time(s) 0.0562\n",
      "Epoch 00016 | Loss 1.9137 | Test Acc 0.5700 | Time(s) 0.0561\n",
      "Epoch 00017 | Loss 1.9115 | Test Acc 0.5790 | Time(s) 0.0560\n",
      "Epoch 00018 | Loss 1.9094 | Test Acc 0.5960 | Time(s) 0.0561\n",
      "Epoch 00019 | Loss 1.9072 | Test Acc 0.6060 | Time(s) 0.0561\n",
      "Epoch 00020 | Loss 1.9050 | Test Acc 0.6150 | Time(s) 0.0560\n",
      "Epoch 00021 | Loss 1.9028 | Test Acc 0.6320 | Time(s) 0.0561\n",
      "Epoch 00022 | Loss 1.9006 | Test Acc 0.6440 | Time(s) 0.0561\n",
      "Epoch 00023 | Loss 1.8984 | Test Acc 0.6480 | Time(s) 0.0560\n",
      "Epoch 00024 | Loss 1.8962 | Test Acc 0.6540 | Time(s) 0.0561\n",
      "Epoch 00025 | Loss 1.8940 | Test Acc 0.6620 | Time(s) 0.0560\n",
      "Epoch 00026 | Loss 1.8917 | Test Acc 0.6640 | Time(s) 0.0559\n",
      "Epoch 00027 | Loss 1.8895 | Test Acc 0.6650 | Time(s) 0.0560\n",
      "Epoch 00028 | Loss 1.8872 | Test Acc 0.6690 | Time(s) 0.0560\n",
      "Epoch 00029 | Loss 1.8849 | Test Acc 0.6740 | Time(s) 0.0559\n",
      "Epoch 00030 | Loss 1.8826 | Test Acc 0.6780 | Time(s) 0.0559\n",
      "Epoch 00031 | Loss 1.8803 | Test Acc 0.6810 | Time(s) 0.0559\n",
      "Epoch 00032 | Loss 1.8780 | Test Acc 0.6850 | Time(s) 0.0559\n",
      "Epoch 00033 | Loss 1.8757 | Test Acc 0.6880 | Time(s) 0.0559\n",
      "Epoch 00034 | Loss 1.8733 | Test Acc 0.6950 | Time(s) 0.0558\n",
      "Epoch 00035 | Loss 1.8710 | Test Acc 0.6990 | Time(s) 0.0558\n",
      "Epoch 00036 | Loss 1.8686 | Test Acc 0.7040 | Time(s) 0.0558\n",
      "Epoch 00037 | Loss 1.8662 | Test Acc 0.7080 | Time(s) 0.0558\n",
      "Epoch 00038 | Loss 1.8638 | Test Acc 0.7080 | Time(s) 0.0557\n",
      "Epoch 00039 | Loss 1.8614 | Test Acc 0.7100 | Time(s) 0.0558\n",
      "Epoch 00040 | Loss 1.8590 | Test Acc 0.7110 | Time(s) 0.0558\n",
      "Epoch 00041 | Loss 1.8565 | Test Acc 0.7180 | Time(s) 0.0558\n",
      "Epoch 00042 | Loss 1.8541 | Test Acc 0.7200 | Time(s) 0.0558\n",
      "Epoch 00043 | Loss 1.8516 | Test Acc 0.7220 | Time(s) 0.0558\n",
      "Epoch 00044 | Loss 1.8491 | Test Acc 0.7220 | Time(s) 0.0558\n",
      "Epoch 00045 | Loss 1.8466 | Test Acc 0.7240 | Time(s) 0.0558\n",
      "Epoch 00046 | Loss 1.8441 | Test Acc 0.7250 | Time(s) 0.0558\n",
      "Epoch 00047 | Loss 1.8415 | Test Acc 0.7250 | Time(s) 0.0557\n",
      "Epoch 00048 | Loss 1.8390 | Test Acc 0.7240 | Time(s) 0.0558\n",
      "Epoch 00049 | Loss 1.8364 | Test Acc 0.7230 | Time(s) 0.0557\n",
      "Epoch 00050 | Loss 1.8338 | Test Acc 0.7270 | Time(s) 0.0557\n",
      "Epoch 00051 | Loss 1.8312 | Test Acc 0.7280 | Time(s) 0.0557\n",
      "Epoch 00052 | Loss 1.8286 | Test Acc 0.7290 | Time(s) 0.0557\n",
      "Epoch 00053 | Loss 1.8260 | Test Acc 0.7310 | Time(s) 0.0557\n",
      "Epoch 00054 | Loss 1.8234 | Test Acc 0.7320 | Time(s) 0.0557\n",
      "Epoch 00055 | Loss 1.8207 | Test Acc 0.7340 | Time(s) 0.0557\n",
      "Epoch 00056 | Loss 1.8180 | Test Acc 0.7340 | Time(s) 0.0557\n",
      "Epoch 00057 | Loss 1.8153 | Test Acc 0.7330 | Time(s) 0.0557\n",
      "Epoch 00058 | Loss 1.8126 | Test Acc 0.7340 | Time(s) 0.0557\n",
      "Epoch 00059 | Loss 1.8099 | Test Acc 0.7330 | Time(s) 0.0557\n",
      "Epoch 00060 | Loss 1.8072 | Test Acc 0.7340 | Time(s) 0.0557\n",
      "Epoch 00061 | Loss 1.8044 | Test Acc 0.7340 | Time(s) 0.0557\n",
      "Epoch 00062 | Loss 1.8016 | Test Acc 0.7350 | Time(s) 0.0557\n",
      "Epoch 00063 | Loss 1.7988 | Test Acc 0.7360 | Time(s) 0.0557\n",
      "Epoch 00064 | Loss 1.7960 | Test Acc 0.7380 | Time(s) 0.0557\n",
      "Epoch 00065 | Loss 1.7932 | Test Acc 0.7380 | Time(s) 0.0557\n",
      "Epoch 00066 | Loss 1.7904 | Test Acc 0.7380 | Time(s) 0.0558\n",
      "Epoch 00067 | Loss 1.7875 | Test Acc 0.7380 | Time(s) 0.0558\n",
      "Epoch 00068 | Loss 1.7847 | Test Acc 0.7390 | Time(s) 0.0557\n",
      "Epoch 00069 | Loss 1.7818 | Test Acc 0.7400 | Time(s) 0.0558\n",
      "Epoch 00070 | Loss 1.7789 | Test Acc 0.7410 | Time(s) 0.0558\n",
      "Epoch 00071 | Loss 1.7759 | Test Acc 0.7420 | Time(s) 0.0557\n",
      "Epoch 00072 | Loss 1.7730 | Test Acc 0.7420 | Time(s) 0.0558\n",
      "Epoch 00073 | Loss 1.7700 | Test Acc 0.7420 | Time(s) 0.0558\n",
      "Epoch 00074 | Loss 1.7671 | Test Acc 0.7430 | Time(s) 0.0557\n",
      "Epoch 00075 | Loss 1.7641 | Test Acc 0.7440 | Time(s) 0.0558\n",
      "Epoch 00076 | Loss 1.7611 | Test Acc 0.7450 | Time(s) 0.0558\n",
      "Epoch 00077 | Loss 1.7581 | Test Acc 0.7470 | Time(s) 0.0558\n",
      "Epoch 00078 | Loss 1.7550 | Test Acc 0.7490 | Time(s) 0.0558\n",
      "Epoch 00079 | Loss 1.7520 | Test Acc 0.7490 | Time(s) 0.0558\n",
      "Epoch 00080 | Loss 1.7489 | Test Acc 0.7500 | Time(s) 0.0558\n",
      "Epoch 00081 | Loss 1.7458 | Test Acc 0.7510 | Time(s) 0.0558\n",
      "Epoch 00082 | Loss 1.7427 | Test Acc 0.7520 | Time(s) 0.0558\n",
      "Epoch 00083 | Loss 1.7396 | Test Acc 0.7520 | Time(s) 0.0558\n",
      "Epoch 00084 | Loss 1.7364 | Test Acc 0.7540 | Time(s) 0.0558\n",
      "Epoch 00085 | Loss 1.7333 | Test Acc 0.7550 | Time(s) 0.0558\n",
      "Epoch 00086 | Loss 1.7301 | Test Acc 0.7550 | Time(s) 0.0557\n",
      "Epoch 00087 | Loss 1.7269 | Test Acc 0.7550 | Time(s) 0.0558\n",
      "Epoch 00088 | Loss 1.7237 | Test Acc 0.7550 | Time(s) 0.0557\n",
      "Epoch 00089 | Loss 1.7205 | Test Acc 0.7550 | Time(s) 0.0557\n",
      "Epoch 00090 | Loss 1.7172 | Test Acc 0.7580 | Time(s) 0.0557\n",
      "Epoch 00091 | Loss 1.7140 | Test Acc 0.7590 | Time(s) 0.0557\n",
      "Epoch 00092 | Loss 1.7107 | Test Acc 0.7610 | Time(s) 0.0557\n",
      "Epoch 00093 | Loss 1.7074 | Test Acc 0.7610 | Time(s) 0.0558\n",
      "Epoch 00094 | Loss 1.7041 | Test Acc 0.7610 | Time(s) 0.0558\n",
      "Epoch 00095 | Loss 1.7008 | Test Acc 0.7600 | Time(s) 0.0557\n",
      "Epoch 00096 | Loss 1.6975 | Test Acc 0.7610 | Time(s) 0.0558\n",
      "Epoch 00097 | Loss 1.6941 | Test Acc 0.7610 | Time(s) 0.0557\n",
      "Epoch 00098 | Loss 1.6907 | Test Acc 0.7620 | Time(s) 0.0557\n",
      "Epoch 00099 | Loss 1.6873 | Test Acc 0.7640 | Time(s) 0.0558\n"
     ]
    }
   ],
   "source": [
    "net = GAT(g, in_dim=features.size()[1], hidden_dim=8, out_dim=7, num_heads=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "dur = []\n",
    "for epoch in range(100):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    logits = net(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\n",
    "        \"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a474e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
