{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6c4cc3",
   "metadata": {},
   "source": [
    "# IIC-3641 GML UC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c409ab",
   "metadata": {},
   "source": [
    "- Versiones de librerías, python 3.10.2\n",
    "- DGL: https://www.dgl.ai/pages/start.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fe5d1d-83e7-4998-93f4-7a577b383bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976c1a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8225f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "print(dgl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f683396",
   "metadata": {},
   "source": [
    "## DGL requiere de un framework de backend. Aquí va con torch sobre cuda (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5284e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ceb47d",
   "metadata": {},
   "source": [
    "## Definiremos una capa R-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd3f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        num_rels,\n",
    "        num_bases=-1, # esto indica que num_bases = num_rels\n",
    "        bias=None,\n",
    "        activation=None,\n",
    "        is_input_layer=False,\n",
    "    ):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases # implementación eficiente de relaciones con combinación lineal de matrices base.\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        \n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(self.num_bases, self.in_feat, self.out_feat)\n",
    "        )\n",
    "        if self.num_bases < self.num_rels:\n",
    "            \n",
    "            self.w_comp = nn.Parameter(\n",
    "                torch.Tensor(self.num_rels, self.num_bases)\n",
    "            )\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(\n",
    "            self.weight, gain=nn.init.calculate_gain(\"relu\")\n",
    "        )\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(\n",
    "                self.w_comp, gain=nn.init.calculate_gain(\"relu\")\n",
    "            )\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(\n",
    "                self.bias, gain=nn.init.calculate_gain(\"relu\")\n",
    "            )\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:\n",
    "            weight = self.weight.view(\n",
    "                self.in_feat, self.num_bases, self.out_feat\n",
    "            )\n",
    "            weight = torch.matmul(self.w_comp, weight).view(\n",
    "                self.num_rels, self.in_feat, self.out_feat\n",
    "            )\n",
    "        else:\n",
    "            weight = self.weight\n",
    "        if self.is_input_layer:\n",
    "\n",
    "            def message_func(edges):\n",
    "                embed = weight.view(-1, self.out_feat)\n",
    "                index = edges.data[dgl.ETYPE] * self.in_feat + edges.src[\"id\"]\n",
    "                return {\"msg\": embed[index] * edges.data[\"norm\"]}\n",
    "\n",
    "        else:\n",
    "\n",
    "            def message_func(edges):\n",
    "                w = weight[edges.data[dgl.ETYPE]]\n",
    "                msg = torch.bmm(edges.src[\"h\"].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data[\"norm\"]\n",
    "                return {\"msg\": msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data[\"h\"]\n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {\"h\": h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg=\"msg\", out=\"h\"), apply_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742391a",
   "metadata": {},
   "source": [
    "## Definimos la R-GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a73ee799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes,\n",
    "        h_dim,\n",
    "        out_dim,\n",
    "        num_rels,\n",
    "        num_bases=-1,\n",
    "        num_hidden_layers=1,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.num_nodes,\n",
    "            self.h_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=F.relu,\n",
    "            is_input_layer=True,\n",
    "        )\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.h_dim,\n",
    "            self.h_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=F.relu,\n",
    "        )\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.h_dim,\n",
    "            self.out_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=partial(F.softmax, dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.features is not None:\n",
    "            g.ndata[\"id\"] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop(\"h\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cd5bf",
   "metadata": {},
   "source": [
    "## Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6296406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# load graph data (Facebook)\n",
    "\n",
    "dataset = dgl.data.rdf.AIFBDataset()\n",
    "g = dataset[0]\n",
    "category = dataset.predict_category\n",
    "train_mask = g.nodes[category].data.pop(\"train_mask\")\n",
    "test_mask = g.nodes[category].data.pop(\"test_mask\")\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "labels = g.nodes[category].data.pop(\"label\")\n",
    "num_rels = len(g.canonical_etypes)\n",
    "num_classes = dataset.num_classes\n",
    "# normalization factor\n",
    "for cetype in g.canonical_etypes:\n",
    "    g.edges[cetype].data[\"norm\"] = dgl.norm_by_dst(g, cetype).unsqueeze(1)\n",
    "category_id = g.ntypes.index(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e205eb0",
   "metadata": {},
   "source": [
    "## Creamos el grafo y el modelo. El grafo se pasa a homogéneo y se agrega la data de las relaciones a los enlaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21264eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=7262, num_edges=48810,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'norm': Scheme(shape=(1,), dtype=torch.float32), '_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "# configurations\n",
    "n_hidden = 16  # number of hidden units\n",
    "n_bases = -1  # use number of relations as number of bases\n",
    "n_hidden_layers = 0  # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 50  # epochs to train\n",
    "lr = 0.01  # learning rate\n",
    "l2norm = 0  # L2 norm coefficient\n",
    "\n",
    "g = dgl.to_homogeneous(g, edata=[\"norm\"])\n",
    "node_ids = torch.arange(g.num_nodes())\n",
    "target_idx = node_ids[g.ndata[dgl.NTYPE] == category_id]\n",
    "\n",
    "model = Model(\n",
    "    g.num_nodes(),\n",
    "    n_hidden,\n",
    "    num_classes,\n",
    "    num_rels,\n",
    "    num_bases=n_bases,\n",
    "    num_hidden_layers=n_hidden_layers,\n",
    ")\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc3203",
   "metadata": {},
   "source": [
    "## Definimos el ciclo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9964ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.1071 | Train Loss: 1.3864 | Validation Accuracy: 0.1389 | Validation loss: 1.3865\n",
      "Epoch 00001 | Train Accuracy: 0.9429 | Train Loss: 1.3547 | Validation Accuracy: 0.9167 | Validation loss: 1.3616\n",
      "Epoch 00002 | Train Accuracy: 0.9500 | Train Loss: 1.3066 | Validation Accuracy: 0.9167 | Validation loss: 1.3227\n",
      "Epoch 00003 | Train Accuracy: 0.9500 | Train Loss: 1.2421 | Validation Accuracy: 0.9167 | Validation loss: 1.2699\n",
      "Epoch 00004 | Train Accuracy: 0.9500 | Train Loss: 1.1697 | Validation Accuracy: 0.9167 | Validation loss: 1.2094\n",
      "Epoch 00005 | Train Accuracy: 0.9429 | Train Loss: 1.1004 | Validation Accuracy: 0.9167 | Validation loss: 1.1511\n",
      "Epoch 00006 | Train Accuracy: 0.9357 | Train Loss: 1.0404 | Validation Accuracy: 0.9167 | Validation loss: 1.1001\n",
      "Epoch 00007 | Train Accuracy: 0.9357 | Train Loss: 0.9909 | Validation Accuracy: 0.9444 | Validation loss: 1.0560\n",
      "Epoch 00008 | Train Accuracy: 0.9429 | Train Loss: 0.9508 | Validation Accuracy: 0.9444 | Validation loss: 1.0177\n",
      "Epoch 00009 | Train Accuracy: 0.9429 | Train Loss: 0.9188 | Validation Accuracy: 0.9444 | Validation loss: 0.9847\n",
      "Epoch 00010 | Train Accuracy: 0.9429 | Train Loss: 0.8929 | Validation Accuracy: 0.9444 | Validation loss: 0.9569\n",
      "Epoch 00011 | Train Accuracy: 0.9500 | Train Loss: 0.8719 | Validation Accuracy: 0.9444 | Validation loss: 0.9335\n",
      "Epoch 00012 | Train Accuracy: 0.9571 | Train Loss: 0.8548 | Validation Accuracy: 0.9444 | Validation loss: 0.9140\n",
      "Epoch 00013 | Train Accuracy: 0.9571 | Train Loss: 0.8408 | Validation Accuracy: 0.9444 | Validation loss: 0.8981\n",
      "Epoch 00014 | Train Accuracy: 0.9571 | Train Loss: 0.8295 | Validation Accuracy: 0.9444 | Validation loss: 0.8853\n",
      "Epoch 00015 | Train Accuracy: 0.9571 | Train Loss: 0.8204 | Validation Accuracy: 0.9444 | Validation loss: 0.8751\n",
      "Epoch 00016 | Train Accuracy: 0.9571 | Train Loss: 0.8133 | Validation Accuracy: 0.9444 | Validation loss: 0.8670\n",
      "Epoch 00017 | Train Accuracy: 0.9571 | Train Loss: 0.8079 | Validation Accuracy: 0.9444 | Validation loss: 0.8606\n",
      "Epoch 00018 | Train Accuracy: 0.9571 | Train Loss: 0.8038 | Validation Accuracy: 0.9444 | Validation loss: 0.8554\n",
      "Epoch 00019 | Train Accuracy: 0.9571 | Train Loss: 0.8006 | Validation Accuracy: 0.9444 | Validation loss: 0.8511\n",
      "Epoch 00020 | Train Accuracy: 0.9571 | Train Loss: 0.7981 | Validation Accuracy: 0.9722 | Validation loss: 0.8475\n",
      "Epoch 00021 | Train Accuracy: 0.9571 | Train Loss: 0.7959 | Validation Accuracy: 0.9722 | Validation loss: 0.8444\n",
      "Epoch 00022 | Train Accuracy: 0.9571 | Train Loss: 0.7939 | Validation Accuracy: 0.9722 | Validation loss: 0.8416\n",
      "Epoch 00023 | Train Accuracy: 0.9571 | Train Loss: 0.7920 | Validation Accuracy: 0.9722 | Validation loss: 0.8392\n",
      "Epoch 00024 | Train Accuracy: 0.9571 | Train Loss: 0.7900 | Validation Accuracy: 0.9722 | Validation loss: 0.8370\n",
      "Epoch 00025 | Train Accuracy: 0.9571 | Train Loss: 0.7879 | Validation Accuracy: 0.9722 | Validation loss: 0.8351\n",
      "Epoch 00026 | Train Accuracy: 0.9571 | Train Loss: 0.7855 | Validation Accuracy: 0.9722 | Validation loss: 0.8332\n",
      "Epoch 00027 | Train Accuracy: 0.9571 | Train Loss: 0.7829 | Validation Accuracy: 0.9722 | Validation loss: 0.8316\n",
      "Epoch 00028 | Train Accuracy: 0.9643 | Train Loss: 0.7801 | Validation Accuracy: 0.9722 | Validation loss: 0.8301\n",
      "Epoch 00029 | Train Accuracy: 0.9786 | Train Loss: 0.7771 | Validation Accuracy: 0.9722 | Validation loss: 0.8288\n",
      "Epoch 00030 | Train Accuracy: 0.9857 | Train Loss: 0.7742 | Validation Accuracy: 0.9722 | Validation loss: 0.8276\n",
      "Epoch 00031 | Train Accuracy: 0.9857 | Train Loss: 0.7716 | Validation Accuracy: 0.9722 | Validation loss: 0.8265\n",
      "Epoch 00032 | Train Accuracy: 0.9857 | Train Loss: 0.7693 | Validation Accuracy: 0.9722 | Validation loss: 0.8256\n",
      "Epoch 00033 | Train Accuracy: 0.9857 | Train Loss: 0.7674 | Validation Accuracy: 0.9722 | Validation loss: 0.8247\n",
      "Epoch 00034 | Train Accuracy: 0.9857 | Train Loss: 0.7660 | Validation Accuracy: 0.9722 | Validation loss: 0.8239\n",
      "Epoch 00035 | Train Accuracy: 0.9857 | Train Loss: 0.7648 | Validation Accuracy: 0.9722 | Validation loss: 0.8232\n",
      "Epoch 00036 | Train Accuracy: 0.9857 | Train Loss: 0.7639 | Validation Accuracy: 0.9722 | Validation loss: 0.8225\n",
      "Epoch 00037 | Train Accuracy: 0.9857 | Train Loss: 0.7632 | Validation Accuracy: 0.9722 | Validation loss: 0.8218\n",
      "Epoch 00038 | Train Accuracy: 0.9857 | Train Loss: 0.7626 | Validation Accuracy: 0.9722 | Validation loss: 0.8212\n",
      "Epoch 00039 | Train Accuracy: 0.9857 | Train Loss: 0.7620 | Validation Accuracy: 0.9722 | Validation loss: 0.8205\n",
      "Epoch 00040 | Train Accuracy: 0.9857 | Train Loss: 0.7616 | Validation Accuracy: 0.9722 | Validation loss: 0.8198\n",
      "Epoch 00041 | Train Accuracy: 0.9857 | Train Loss: 0.7612 | Validation Accuracy: 0.9722 | Validation loss: 0.8192\n",
      "Epoch 00042 | Train Accuracy: 0.9857 | Train Loss: 0.7609 | Validation Accuracy: 0.9722 | Validation loss: 0.8185\n",
      "Epoch 00043 | Train Accuracy: 0.9857 | Train Loss: 0.7606 | Validation Accuracy: 0.9722 | Validation loss: 0.8178\n",
      "Epoch 00044 | Train Accuracy: 0.9857 | Train Loss: 0.7603 | Validation Accuracy: 0.9722 | Validation loss: 0.8172\n",
      "Epoch 00045 | Train Accuracy: 0.9857 | Train Loss: 0.7601 | Validation Accuracy: 0.9722 | Validation loss: 0.8165\n",
      "Epoch 00046 | Train Accuracy: 0.9857 | Train Loss: 0.7599 | Validation Accuracy: 0.9722 | Validation loss: 0.8159\n",
      "Epoch 00047 | Train Accuracy: 0.9857 | Train Loss: 0.7597 | Validation Accuracy: 0.9722 | Validation loss: 0.8152\n",
      "Epoch 00048 | Train Accuracy: 0.9857 | Train Loss: 0.7595 | Validation Accuracy: 0.9722 | Validation loss: 0.8146\n",
      "Epoch 00049 | Train Accuracy: 0.9857 | Train Loss: 0.7593 | Validation Accuracy: 0.9722 | Validation loss: 0.8141\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    logits = logits[target_idx]\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "    val_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx])\n",
    "    val_acc = val_acc.item() / len(test_idx)\n",
    "    print(\n",
    "        \"Epoch {:05d} | \".format(epoch)\n",
    "        + \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "            train_acc, loss.item()\n",
    "        )\n",
    "        + \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "            val_acc, val_loss.item()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdef9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
