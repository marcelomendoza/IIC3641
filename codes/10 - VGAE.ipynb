{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d98f489",
   "metadata": {},
   "source": [
    "# IIC-3641 GML UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4031983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dda7d0",
   "metadata": {},
   "source": [
    "## Aquí leemos los datos, los cuales están en la carpeta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1127cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix, dia_matrix, lil_matrix, eye, vstack, isspmatrix_coo, coo_matrix, diags, triu\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_data(dataset):\n",
    "    # load the data: x, tx, allx, graph\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "    features = vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c6b7e",
   "metadata": {},
   "source": [
    "## Aquí se definen funciones que son útiles para trabajar con los datos, por ejemplo preprocesar el grafo para crear la matriz de adyacencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2503025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = coo_matrix(adj)\n",
    "    adj_ = adj + eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 10% positive links\n",
    "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "    # TODO: Clean up.\n",
    "\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b8380d",
   "metadata": {},
   "source": [
    "## Trabajamos con CORA, input_dim esta definido por la dimensionalidad de los datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bcde805",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cora'\n",
    "model = 'VGAE'\n",
    "\n",
    "input_dim = 1433 \n",
    "hidden1_dim = 32\n",
    "hidden2_dim = 16\n",
    "use_feature = True\n",
    "\n",
    "num_epoch = 200\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e847844",
   "metadata": {},
   "source": [
    "## Aquí se define el VGAE, implementado sobre capas convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27113ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class VGAE(nn.Module):\n",
    "    def __init__(self, adj):\n",
    "        super(VGAE,self).__init__()\n",
    "        self.base_gcn = GraphConvSparse(input_dim, hidden1_dim, adj)\n",
    "        self.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, adj, activation=lambda x:x)\n",
    "        self.gcn_logstddev = GraphConvSparse(hidden1_dim, hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "    def encode(self, X):\n",
    "        hidden = self.base_gcn(X)\n",
    "        self.mean = self.gcn_mean(hidden)\n",
    "        self.logstd = self.gcn_logstddev(hidden)\n",
    "        gaussian_noise = torch.randn(X.size(0), hidden2_dim)\n",
    "        sampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "        return sampled_z\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.encode(X)\n",
    "        A_pred = dot_product_decode(Z)\n",
    "        return A_pred\n",
    "\n",
    "    \n",
    "class GraphConvSparse(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, adj, activation = F.relu, **kwargs):\n",
    "        super(GraphConvSparse, self).__init__(**kwargs)\n",
    "        self.weight = glorot_init(input_dim, output_dim) \n",
    "        self.adj = adj\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.mm(x,self.weight)\n",
    "        x = torch.mm(self.adj, x)\n",
    "        outputs = self.activation(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "    A_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "    return A_pred\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "    init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "    initial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "    return nn.Parameter(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07e0af",
   "metadata": {},
   "source": [
    "## Aquí leemos y preprocesamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f8330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_145710/276524816.py:20: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import time\n",
    "\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "adj, features = load_data(dataset)\n",
    "\n",
    "A = adj\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18f9e0",
   "metadata": {},
   "source": [
    "## Entran en la VGAE A y X. En el caso de A entra nomalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edfe3bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_145710/503417153.py:8: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:641.)\n",
      "  adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T),\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "adj_label = adj_train + eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                            torch.FloatTensor(features[1]), \n",
    "                            torch.Size(features[2]))\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e401cd",
   "metadata": {},
   "source": [
    "## La implementación pasa A al constructor. X entra en el forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da34477",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGAE(adj_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88350378",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4b84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        # print(e)\n",
    "        # print(adj_rec[e[0], e[1]])\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score\n",
    "\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b88b9e",
   "metadata": {},
   "source": [
    "## El ciclo de entrenamiento usa la función de pérdida vista en clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25dec9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.79959 train_acc= 0.49588 val_roc= 0.49217 val_ap= 0.53069 time= 0.13733\n",
      "Epoch: 0002 train_loss= 1.47549 train_acc= 0.47979 val_roc= 0.49884 val_ap= 0.50028 time= 0.10144\n",
      "Epoch: 0003 train_loss= 1.30103 train_acc= 0.44440 val_roc= 0.53796 val_ap= 0.51577 time= 0.10317\n",
      "Epoch: 0004 train_loss= 1.11318 train_acc= 0.39691 val_roc= 0.53972 val_ap= 0.55298 time= 0.09967\n",
      "Epoch: 0005 train_loss= 1.00111 train_acc= 0.35330 val_roc= 0.55126 val_ap= 0.52792 time= 0.09593\n",
      "Epoch: 0006 train_loss= 0.89926 train_acc= 0.31061 val_roc= 0.58601 val_ap= 0.56507 time= 0.09684\n",
      "Epoch: 0007 train_loss= 0.79474 train_acc= 0.31727 val_roc= 0.58888 val_ap= 0.57015 time= 0.09250\n",
      "Epoch: 0008 train_loss= 0.74906 train_acc= 0.34623 val_roc= 0.60955 val_ap= 0.60040 time= 0.10069\n",
      "Epoch: 0009 train_loss= 0.72238 train_acc= 0.36476 val_roc= 0.58130 val_ap= 0.55456 time= 0.10046\n",
      "Epoch: 0010 train_loss= 0.71193 train_acc= 0.29284 val_roc= 0.59985 val_ap= 0.59670 time= 0.10168\n",
      "Epoch: 0011 train_loss= 0.70740 train_acc= 0.17100 val_roc= 0.63903 val_ap= 0.63381 time= 0.10461\n",
      "Epoch: 0012 train_loss= 0.70370 train_acc= 0.12359 val_roc= 0.66718 val_ap= 0.67376 time= 0.10574\n",
      "Epoch: 0013 train_loss= 0.68519 train_acc= 0.15094 val_roc= 0.71526 val_ap= 0.73101 time= 0.09817\n",
      "Epoch: 0014 train_loss= 0.66744 train_acc= 0.21415 val_roc= 0.73192 val_ap= 0.73422 time= 0.09510\n",
      "Epoch: 0015 train_loss= 0.64967 train_acc= 0.26910 val_roc= 0.75519 val_ap= 0.74804 time= 0.10081\n",
      "Epoch: 0016 train_loss= 0.63143 train_acc= 0.30198 val_roc= 0.77207 val_ap= 0.76247 time= 0.10606\n",
      "Epoch: 0017 train_loss= 0.61258 train_acc= 0.35185 val_roc= 0.79669 val_ap= 0.77562 time= 0.10002\n",
      "Epoch: 0018 train_loss= 0.59439 train_acc= 0.39994 val_roc= 0.80535 val_ap= 0.78843 time= 0.09163\n",
      "Epoch: 0019 train_loss= 0.57880 train_acc= 0.43811 val_roc= 0.80170 val_ap= 0.77916 time= 0.09235\n",
      "Epoch: 0020 train_loss= 0.57285 train_acc= 0.45135 val_roc= 0.81680 val_ap= 0.79078 time= 0.09217\n",
      "Epoch: 0021 train_loss= 0.57151 train_acc= 0.46154 val_roc= 0.81240 val_ap= 0.79006 time= 0.09718\n",
      "Epoch: 0022 train_loss= 0.57102 train_acc= 0.46820 val_roc= 0.82579 val_ap= 0.80552 time= 0.09414\n",
      "Epoch: 0023 train_loss= 0.56554 train_acc= 0.47700 val_roc= 0.83211 val_ap= 0.81529 time= 0.09575\n",
      "Epoch: 0024 train_loss= 0.55636 train_acc= 0.49026 val_roc= 0.83694 val_ap= 0.81199 time= 0.09268\n",
      "Epoch: 0025 train_loss= 0.54546 train_acc= 0.49504 val_roc= 0.83237 val_ap= 0.81052 time= 0.09572\n",
      "Epoch: 0026 train_loss= 0.53718 train_acc= 0.49725 val_roc= 0.84156 val_ap= 0.82044 time= 0.09782\n",
      "Epoch: 0027 train_loss= 0.53358 train_acc= 0.49637 val_roc= 0.84250 val_ap= 0.82121 time= 0.10171\n",
      "Epoch: 0028 train_loss= 0.53143 train_acc= 0.49821 val_roc= 0.84347 val_ap= 0.82502 time= 0.09722\n",
      "Epoch: 0029 train_loss= 0.52957 train_acc= 0.50003 val_roc= 0.84304 val_ap= 0.82862 time= 0.09080\n",
      "Epoch: 0030 train_loss= 0.52945 train_acc= 0.50368 val_roc= 0.84249 val_ap= 0.82713 time= 0.09063\n",
      "Epoch: 0031 train_loss= 0.52670 train_acc= 0.50816 val_roc= 0.83584 val_ap= 0.82529 time= 0.09340\n",
      "Epoch: 0032 train_loss= 0.52333 train_acc= 0.51008 val_roc= 0.83997 val_ap= 0.83430 time= 0.09608\n",
      "Epoch: 0033 train_loss= 0.52136 train_acc= 0.50776 val_roc= 0.84114 val_ap= 0.83811 time= 0.09184\n",
      "Epoch: 0034 train_loss= 0.51893 train_acc= 0.50170 val_roc= 0.85037 val_ap= 0.84902 time= 0.09493\n",
      "Epoch: 0035 train_loss= 0.51548 train_acc= 0.50569 val_roc= 0.85209 val_ap= 0.85058 time= 0.10052\n",
      "Epoch: 0036 train_loss= 0.50953 train_acc= 0.51299 val_roc= 0.84979 val_ap= 0.84941 time= 0.11919\n",
      "Epoch: 0037 train_loss= 0.50496 train_acc= 0.51900 val_roc= 0.84940 val_ap= 0.85382 time= 0.10304\n",
      "Epoch: 0038 train_loss= 0.50024 train_acc= 0.52205 val_roc= 0.86890 val_ap= 0.87240 time= 0.09877\n",
      "Epoch: 0039 train_loss= 0.49822 train_acc= 0.52135 val_roc= 0.86005 val_ap= 0.86611 time= 0.10861\n",
      "Epoch: 0040 train_loss= 0.49610 train_acc= 0.52065 val_roc= 0.87090 val_ap= 0.87795 time= 0.10364\n",
      "Epoch: 0041 train_loss= 0.49450 train_acc= 0.52248 val_roc= 0.87659 val_ap= 0.88154 time= 0.09571\n",
      "Epoch: 0042 train_loss= 0.49325 train_acc= 0.52402 val_roc= 0.87386 val_ap= 0.88019 time= 0.09382\n",
      "Epoch: 0043 train_loss= 0.49092 train_acc= 0.52570 val_roc= 0.87525 val_ap= 0.88285 time= 0.09513\n",
      "Epoch: 0044 train_loss= 0.49044 train_acc= 0.52662 val_roc= 0.87520 val_ap= 0.88209 time= 0.09469\n",
      "Epoch: 0045 train_loss= 0.48961 train_acc= 0.52563 val_roc= 0.87940 val_ap= 0.88293 time= 0.09038\n",
      "Epoch: 0046 train_loss= 0.48762 train_acc= 0.52666 val_roc= 0.88243 val_ap= 0.88620 time= 0.09364\n",
      "Epoch: 0047 train_loss= 0.48626 train_acc= 0.52825 val_roc= 0.87580 val_ap= 0.88132 time= 0.10193\n",
      "Epoch: 0048 train_loss= 0.48354 train_acc= 0.53006 val_roc= 0.88201 val_ap= 0.88515 time= 0.10311\n",
      "Epoch: 0049 train_loss= 0.48241 train_acc= 0.53083 val_roc= 0.88540 val_ap= 0.88798 time= 0.09971\n",
      "Epoch: 0050 train_loss= 0.48104 train_acc= 0.53080 val_roc= 0.88410 val_ap= 0.88578 time= 0.09531\n",
      "Epoch: 0051 train_loss= 0.47999 train_acc= 0.53002 val_roc= 0.88577 val_ap= 0.89135 time= 0.09319\n",
      "Epoch: 0052 train_loss= 0.47880 train_acc= 0.53169 val_roc= 0.88083 val_ap= 0.88274 time= 0.09386\n",
      "Epoch: 0053 train_loss= 0.47829 train_acc= 0.53100 val_roc= 0.88687 val_ap= 0.88917 time= 0.09863\n",
      "Epoch: 0054 train_loss= 0.47751 train_acc= 0.53251 val_roc= 0.87610 val_ap= 0.88423 time= 0.10020\n",
      "Epoch: 0055 train_loss= 0.47669 train_acc= 0.53137 val_roc= 0.88690 val_ap= 0.88851 time= 0.09342\n",
      "Epoch: 0056 train_loss= 0.47554 train_acc= 0.53281 val_roc= 0.87967 val_ap= 0.88285 time= 0.10321\n",
      "Epoch: 0057 train_loss= 0.47474 train_acc= 0.53212 val_roc= 0.88605 val_ap= 0.88548 time= 0.11369\n",
      "Epoch: 0058 train_loss= 0.47453 train_acc= 0.53129 val_roc= 0.89154 val_ap= 0.89272 time= 0.10411\n",
      "Epoch: 0059 train_loss= 0.47288 train_acc= 0.53380 val_roc= 0.88224 val_ap= 0.88365 time= 0.11085\n",
      "Epoch: 0060 train_loss= 0.47219 train_acc= 0.53379 val_roc= 0.89573 val_ap= 0.89700 time= 0.10699\n",
      "Epoch: 0061 train_loss= 0.47103 train_acc= 0.53413 val_roc= 0.88814 val_ap= 0.88737 time= 0.11038\n",
      "Epoch: 0062 train_loss= 0.47063 train_acc= 0.53485 val_roc= 0.89429 val_ap= 0.89482 time= 0.10643\n",
      "Epoch: 0063 train_loss= 0.46984 train_acc= 0.53485 val_roc= 0.89258 val_ap= 0.89387 time= 0.10685\n",
      "Epoch: 0064 train_loss= 0.46905 train_acc= 0.53539 val_roc= 0.89021 val_ap= 0.89223 time= 0.10605\n",
      "Epoch: 0065 train_loss= 0.46886 train_acc= 0.53563 val_roc= 0.90081 val_ap= 0.90283 time= 0.11263\n",
      "Epoch: 0066 train_loss= 0.46757 train_acc= 0.53599 val_roc= 0.89897 val_ap= 0.89452 time= 0.09802\n",
      "Epoch: 0067 train_loss= 0.46692 train_acc= 0.53609 val_roc= 0.89524 val_ap= 0.90175 time= 0.09605\n",
      "Epoch: 0068 train_loss= 0.46679 train_acc= 0.53556 val_roc= 0.90079 val_ap= 0.89948 time= 0.10374\n",
      "Epoch: 0069 train_loss= 0.46635 train_acc= 0.53715 val_roc= 0.90173 val_ap= 0.90397 time= 0.10487\n",
      "Epoch: 0070 train_loss= 0.46526 train_acc= 0.53657 val_roc= 0.89322 val_ap= 0.89438 time= 0.10685\n",
      "Epoch: 0071 train_loss= 0.46486 train_acc= 0.53681 val_roc= 0.89822 val_ap= 0.90027 time= 0.12278\n",
      "Epoch: 0072 train_loss= 0.46372 train_acc= 0.53748 val_roc= 0.89474 val_ap= 0.89561 time= 0.10232\n",
      "Epoch: 0073 train_loss= 0.46331 train_acc= 0.53868 val_roc= 0.90366 val_ap= 0.90421 time= 0.10886\n",
      "Epoch: 0074 train_loss= 0.46264 train_acc= 0.53945 val_roc= 0.89712 val_ap= 0.89645 time= 0.11119\n",
      "Epoch: 0075 train_loss= 0.46242 train_acc= 0.53858 val_roc= 0.89453 val_ap= 0.89694 time= 0.09661\n",
      "Epoch: 0076 train_loss= 0.46187 train_acc= 0.53819 val_roc= 0.91051 val_ap= 0.90551 time= 0.09582\n",
      "Epoch: 0077 train_loss= 0.46149 train_acc= 0.53923 val_roc= 0.89381 val_ap= 0.89175 time= 0.09340\n",
      "Epoch: 0078 train_loss= 0.46104 train_acc= 0.54016 val_roc= 0.90220 val_ap= 0.89809 time= 0.09118\n",
      "Epoch: 0079 train_loss= 0.46044 train_acc= 0.54078 val_roc= 0.90835 val_ap= 0.90317 time= 0.09525\n",
      "Epoch: 0080 train_loss= 0.46008 train_acc= 0.53967 val_roc= 0.90298 val_ap= 0.89926 time= 0.10055\n",
      "Epoch: 0081 train_loss= 0.45893 train_acc= 0.54116 val_roc= 0.90325 val_ap= 0.89794 time= 0.09639\n",
      "Epoch: 0082 train_loss= 0.45920 train_acc= 0.54131 val_roc= 0.90079 val_ap= 0.89380 time= 0.09104\n",
      "Epoch: 0083 train_loss= 0.45789 train_acc= 0.54143 val_roc= 0.90825 val_ap= 0.90432 time= 0.09207\n",
      "Epoch: 0084 train_loss= 0.45749 train_acc= 0.54294 val_roc= 0.90770 val_ap= 0.90676 time= 0.09581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0085 train_loss= 0.45721 train_acc= 0.54330 val_roc= 0.90935 val_ap= 0.90884 time= 0.11126\n",
      "Epoch: 0086 train_loss= 0.45711 train_acc= 0.54260 val_roc= 0.90666 val_ap= 0.90415 time= 0.11248\n",
      "Epoch: 0087 train_loss= 0.45692 train_acc= 0.54322 val_roc= 0.90768 val_ap= 0.90560 time= 0.10059\n",
      "Epoch: 0088 train_loss= 0.45593 train_acc= 0.54248 val_roc= 0.90724 val_ap= 0.90657 time= 0.10583\n",
      "Epoch: 0089 train_loss= 0.45592 train_acc= 0.54410 val_roc= 0.91239 val_ap= 0.90825 time= 0.11226\n",
      "Epoch: 0090 train_loss= 0.45516 train_acc= 0.54516 val_roc= 0.90640 val_ap= 0.90729 time= 0.09353\n",
      "Epoch: 0091 train_loss= 0.45448 train_acc= 0.54539 val_roc= 0.91021 val_ap= 0.90384 time= 0.10388\n",
      "Epoch: 0092 train_loss= 0.45439 train_acc= 0.54502 val_roc= 0.91888 val_ap= 0.91664 time= 0.10172\n",
      "Epoch: 0093 train_loss= 0.45397 train_acc= 0.54423 val_roc= 0.90311 val_ap= 0.90499 time= 0.09719\n",
      "Epoch: 0094 train_loss= 0.45369 train_acc= 0.54449 val_roc= 0.89796 val_ap= 0.90182 time= 0.09381\n",
      "Epoch: 0095 train_loss= 0.45324 train_acc= 0.54466 val_roc= 0.90418 val_ap= 0.90375 time= 0.10129\n",
      "Epoch: 0096 train_loss= 0.45319 train_acc= 0.54588 val_roc= 0.90568 val_ap= 0.90868 time= 0.09640\n",
      "Epoch: 0097 train_loss= 0.45233 train_acc= 0.54470 val_roc= 0.91071 val_ap= 0.91008 time= 0.09112\n",
      "Epoch: 0098 train_loss= 0.45146 train_acc= 0.54453 val_roc= 0.90691 val_ap= 0.90845 time= 0.09533\n",
      "Epoch: 0099 train_loss= 0.45150 train_acc= 0.54551 val_roc= 0.91104 val_ap= 0.91343 time= 0.09399\n",
      "Epoch: 0100 train_loss= 0.45131 train_acc= 0.54727 val_roc= 0.90671 val_ap= 0.90947 time= 0.11277\n",
      "Epoch: 0101 train_loss= 0.45081 train_acc= 0.54552 val_roc= 0.90652 val_ap= 0.91093 time= 0.11407\n",
      "Epoch: 0102 train_loss= 0.45049 train_acc= 0.54393 val_roc= 0.90940 val_ap= 0.90789 time= 0.09994\n",
      "Epoch: 0103 train_loss= 0.45034 train_acc= 0.54602 val_roc= 0.90857 val_ap= 0.91036 time= 0.10284\n",
      "Epoch: 0104 train_loss= 0.45002 train_acc= 0.54618 val_roc= 0.91086 val_ap= 0.91450 time= 0.10018\n",
      "Epoch: 0105 train_loss= 0.44976 train_acc= 0.54622 val_roc= 0.90723 val_ap= 0.90862 time= 0.10663\n",
      "Epoch: 0106 train_loss= 0.44946 train_acc= 0.54468 val_roc= 0.91097 val_ap= 0.91444 time= 0.10092\n",
      "Epoch: 0107 train_loss= 0.44882 train_acc= 0.54635 val_roc= 0.90834 val_ap= 0.91483 time= 0.09891\n",
      "Epoch: 0108 train_loss= 0.44890 train_acc= 0.54530 val_roc= 0.91070 val_ap= 0.91334 time= 0.09629\n",
      "Epoch: 0109 train_loss= 0.44846 train_acc= 0.54461 val_roc= 0.90146 val_ap= 0.90837 time= 0.10492\n",
      "Epoch: 0110 train_loss= 0.44847 train_acc= 0.54501 val_roc= 0.90484 val_ap= 0.91029 time= 0.09868\n",
      "Epoch: 0111 train_loss= 0.44831 train_acc= 0.54494 val_roc= 0.91073 val_ap= 0.91500 time= 0.09654\n",
      "Epoch: 0112 train_loss= 0.44807 train_acc= 0.54530 val_roc= 0.91025 val_ap= 0.91482 time= 0.09552\n",
      "Epoch: 0113 train_loss= 0.44754 train_acc= 0.54438 val_roc= 0.91868 val_ap= 0.92277 time= 0.10223\n",
      "Epoch: 0114 train_loss= 0.44792 train_acc= 0.54447 val_roc= 0.91226 val_ap= 0.91342 time= 0.10102\n",
      "Epoch: 0115 train_loss= 0.44756 train_acc= 0.54516 val_roc= 0.91323 val_ap= 0.91587 time= 0.13713\n",
      "Epoch: 0116 train_loss= 0.44742 train_acc= 0.54595 val_roc= 0.91240 val_ap= 0.91939 time= 0.11778\n",
      "Epoch: 0117 train_loss= 0.44673 train_acc= 0.54546 val_roc= 0.90674 val_ap= 0.91276 time= 0.11473\n",
      "Epoch: 0118 train_loss= 0.44669 train_acc= 0.54413 val_roc= 0.91613 val_ap= 0.92106 time= 0.12409\n",
      "Epoch: 0119 train_loss= 0.44626 train_acc= 0.54623 val_roc= 0.90854 val_ap= 0.91245 time= 0.12616\n",
      "Epoch: 0120 train_loss= 0.44623 train_acc= 0.54481 val_roc= 0.92074 val_ap= 0.92100 time= 0.11932\n",
      "Epoch: 0121 train_loss= 0.44607 train_acc= 0.54593 val_roc= 0.91320 val_ap= 0.91882 time= 0.11745\n",
      "Epoch: 0122 train_loss= 0.44579 train_acc= 0.54617 val_roc= 0.91119 val_ap= 0.91475 time= 0.11507\n",
      "Epoch: 0123 train_loss= 0.44569 train_acc= 0.54562 val_roc= 0.91921 val_ap= 0.92021 time= 0.09839\n",
      "Epoch: 0124 train_loss= 0.44540 train_acc= 0.54612 val_roc= 0.91957 val_ap= 0.92166 time= 0.12202\n",
      "Epoch: 0125 train_loss= 0.44523 train_acc= 0.54517 val_roc= 0.91347 val_ap= 0.91885 time= 0.12345\n",
      "Epoch: 0126 train_loss= 0.44563 train_acc= 0.54560 val_roc= 0.91268 val_ap= 0.91681 time= 0.10646\n",
      "Epoch: 0127 train_loss= 0.44500 train_acc= 0.54622 val_roc= 0.91937 val_ap= 0.92391 time= 0.12129\n",
      "Epoch: 0128 train_loss= 0.44481 train_acc= 0.54648 val_roc= 0.91862 val_ap= 0.92214 time= 0.12012\n",
      "Epoch: 0129 train_loss= 0.44462 train_acc= 0.54546 val_roc= 0.92086 val_ap= 0.92232 time= 0.10706\n",
      "Epoch: 0130 train_loss= 0.44441 train_acc= 0.54601 val_roc= 0.91331 val_ap= 0.91391 time= 0.10430\n",
      "Epoch: 0131 train_loss= 0.44435 train_acc= 0.54502 val_roc= 0.91545 val_ap= 0.92002 time= 0.11298\n",
      "Epoch: 0132 train_loss= 0.44417 train_acc= 0.54607 val_roc= 0.90717 val_ap= 0.91237 time= 0.10669\n",
      "Epoch: 0133 train_loss= 0.44403 train_acc= 0.54574 val_roc= 0.91687 val_ap= 0.92154 time= 0.12515\n",
      "Epoch: 0134 train_loss= 0.44416 train_acc= 0.54656 val_roc= 0.91227 val_ap= 0.91556 time= 0.11265\n",
      "Epoch: 0135 train_loss= 0.44367 train_acc= 0.54523 val_roc= 0.91300 val_ap= 0.91622 time= 0.12740\n",
      "Epoch: 0136 train_loss= 0.44383 train_acc= 0.54526 val_roc= 0.92021 val_ap= 0.91981 time= 0.10029\n",
      "Epoch: 0137 train_loss= 0.44369 train_acc= 0.54512 val_roc= 0.91920 val_ap= 0.92429 time= 0.10729\n",
      "Epoch: 0138 train_loss= 0.44334 train_acc= 0.54687 val_roc= 0.91422 val_ap= 0.91591 time= 0.09530\n",
      "Epoch: 0139 train_loss= 0.44323 train_acc= 0.54502 val_roc= 0.91263 val_ap= 0.92009 time= 0.09492\n",
      "Epoch: 0140 train_loss= 0.44327 train_acc= 0.54562 val_roc= 0.91568 val_ap= 0.91789 time= 0.09073\n",
      "Epoch: 0141 train_loss= 0.44310 train_acc= 0.54640 val_roc= 0.91242 val_ap= 0.91892 time= 0.09177\n",
      "Epoch: 0142 train_loss= 0.44284 train_acc= 0.54579 val_roc= 0.91369 val_ap= 0.91934 time= 0.09121\n",
      "Epoch: 0143 train_loss= 0.44237 train_acc= 0.54607 val_roc= 0.91979 val_ap= 0.92402 time= 0.09444\n",
      "Epoch: 0144 train_loss= 0.44246 train_acc= 0.54729 val_roc= 0.92197 val_ap= 0.92764 time= 0.10034\n",
      "Epoch: 0145 train_loss= 0.44257 train_acc= 0.54600 val_roc= 0.91712 val_ap= 0.92419 time= 0.09984\n",
      "Epoch: 0146 train_loss= 0.44235 train_acc= 0.54632 val_roc= 0.91490 val_ap= 0.92229 time= 0.09954\n",
      "Epoch: 0147 train_loss= 0.44247 train_acc= 0.54600 val_roc= 0.91130 val_ap= 0.91849 time= 0.10090\n",
      "Epoch: 0148 train_loss= 0.44195 train_acc= 0.54651 val_roc= 0.91444 val_ap= 0.92019 time= 0.11013\n",
      "Epoch: 0149 train_loss= 0.44225 train_acc= 0.54585 val_roc= 0.93043 val_ap= 0.93273 time= 0.10732\n",
      "Epoch: 0150 train_loss= 0.44158 train_acc= 0.54690 val_roc= 0.91119 val_ap= 0.91939 time= 0.10272\n",
      "Epoch: 0151 train_loss= 0.44119 train_acc= 0.54687 val_roc= 0.91331 val_ap= 0.91477 time= 0.09663\n",
      "Epoch: 0152 train_loss= 0.44137 train_acc= 0.54752 val_roc= 0.91409 val_ap= 0.92151 time= 0.09870\n",
      "Epoch: 0153 train_loss= 0.44154 train_acc= 0.54727 val_roc= 0.91839 val_ap= 0.92394 time= 0.10058\n",
      "Epoch: 0154 train_loss= 0.44081 train_acc= 0.54762 val_roc= 0.91810 val_ap= 0.92338 time= 0.11533\n",
      "Epoch: 0155 train_loss= 0.44079 train_acc= 0.54768 val_roc= 0.91464 val_ap= 0.92039 time= 0.10187\n",
      "Epoch: 0156 train_loss= 0.44109 train_acc= 0.54780 val_roc= 0.91673 val_ap= 0.92169 time= 0.09420\n",
      "Epoch: 0157 train_loss= 0.44046 train_acc= 0.54804 val_roc= 0.92226 val_ap= 0.92830 time= 0.09832\n",
      "Epoch: 0158 train_loss= 0.44039 train_acc= 0.54749 val_roc= 0.91067 val_ap= 0.91604 time= 0.09725\n",
      "Epoch: 0159 train_loss= 0.44054 train_acc= 0.54812 val_roc= 0.91777 val_ap= 0.92288 time= 0.10021\n",
      "Epoch: 0160 train_loss= 0.44041 train_acc= 0.54813 val_roc= 0.90950 val_ap= 0.91557 time= 0.10068\n",
      "Epoch: 0161 train_loss= 0.44040 train_acc= 0.54780 val_roc= 0.92177 val_ap= 0.92814 time= 0.11505\n",
      "Epoch: 0162 train_loss= 0.44006 train_acc= 0.54843 val_roc= 0.91956 val_ap= 0.92646 time= 0.10133\n",
      "Epoch: 0163 train_loss= 0.44016 train_acc= 0.54787 val_roc= 0.91848 val_ap= 0.92581 time= 0.09621\n",
      "Epoch: 0164 train_loss= 0.43974 train_acc= 0.54916 val_roc= 0.92057 val_ap= 0.92279 time= 0.10341\n",
      "Epoch: 0165 train_loss= 0.43959 train_acc= 0.54891 val_roc= 0.91130 val_ap= 0.91735 time= 0.09681\n",
      "Epoch: 0166 train_loss= 0.43970 train_acc= 0.54902 val_roc= 0.91641 val_ap= 0.92250 time= 0.11749\n",
      "Epoch: 0167 train_loss= 0.43974 train_acc= 0.54828 val_roc= 0.91495 val_ap= 0.92231 time= 0.11105\n",
      "Epoch: 0168 train_loss= 0.43917 train_acc= 0.54933 val_roc= 0.90509 val_ap= 0.91561 time= 0.12601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0169 train_loss= 0.43951 train_acc= 0.54857 val_roc= 0.91386 val_ap= 0.91822 time= 0.10656\n",
      "Epoch: 0170 train_loss= 0.43993 train_acc= 0.54899 val_roc= 0.91382 val_ap= 0.91719 time= 0.11092\n",
      "Epoch: 0171 train_loss= 0.43902 train_acc= 0.54924 val_roc= 0.92207 val_ap= 0.92692 time= 0.10922\n",
      "Epoch: 0172 train_loss= 0.43922 train_acc= 0.55023 val_roc= 0.92131 val_ap= 0.92589 time= 0.09458\n",
      "Epoch: 0173 train_loss= 0.43885 train_acc= 0.54885 val_roc= 0.91048 val_ap= 0.91593 time= 0.10263\n",
      "Epoch: 0174 train_loss= 0.43892 train_acc= 0.54999 val_roc= 0.91642 val_ap= 0.92464 time= 0.10531\n",
      "Epoch: 0175 train_loss= 0.43882 train_acc= 0.55007 val_roc= 0.92008 val_ap= 0.92404 time= 0.10545\n",
      "Epoch: 0176 train_loss= 0.43846 train_acc= 0.55079 val_roc= 0.91547 val_ap= 0.92022 time= 0.09676\n",
      "Epoch: 0177 train_loss= 0.43855 train_acc= 0.54881 val_roc= 0.91404 val_ap= 0.91847 time= 0.11677\n",
      "Epoch: 0178 train_loss= 0.43872 train_acc= 0.55133 val_roc= 0.90942 val_ap= 0.91527 time= 0.11012\n",
      "Epoch: 0179 train_loss= 0.43860 train_acc= 0.55083 val_roc= 0.91713 val_ap= 0.92320 time= 0.10772\n",
      "Epoch: 0180 train_loss= 0.43847 train_acc= 0.55042 val_roc= 0.91782 val_ap= 0.91993 time= 0.09803\n",
      "Epoch: 0181 train_loss= 0.43843 train_acc= 0.55068 val_roc= 0.92361 val_ap= 0.92731 time= 0.10056\n",
      "Epoch: 0182 train_loss= 0.43858 train_acc= 0.54979 val_roc= 0.91548 val_ap= 0.91794 time= 0.12405\n",
      "Epoch: 0183 train_loss= 0.43844 train_acc= 0.55035 val_roc= 0.90069 val_ap= 0.91160 time= 0.11340\n",
      "Epoch: 0184 train_loss= 0.43810 train_acc= 0.55076 val_roc= 0.91885 val_ap= 0.92531 time= 0.11688\n",
      "Epoch: 0185 train_loss= 0.43836 train_acc= 0.54957 val_roc= 0.91148 val_ap= 0.92115 time= 0.11286\n",
      "Epoch: 0186 train_loss= 0.43788 train_acc= 0.55122 val_roc= 0.91514 val_ap= 0.92120 time= 0.12079\n",
      "Epoch: 0187 train_loss= 0.43813 train_acc= 0.55066 val_roc= 0.91473 val_ap= 0.92075 time= 0.11278\n",
      "Epoch: 0188 train_loss= 0.43811 train_acc= 0.55093 val_roc= 0.91846 val_ap= 0.92327 time= 0.10802\n",
      "Epoch: 0189 train_loss= 0.43780 train_acc= 0.55041 val_roc= 0.91349 val_ap= 0.91672 time= 0.11361\n",
      "Epoch: 0190 train_loss= 0.43744 train_acc= 0.55109 val_roc= 0.91438 val_ap= 0.92144 time= 0.10281\n",
      "Epoch: 0191 train_loss= 0.43794 train_acc= 0.55119 val_roc= 0.91459 val_ap= 0.91911 time= 0.11271\n",
      "Epoch: 0192 train_loss= 0.43782 train_acc= 0.55077 val_roc= 0.90582 val_ap= 0.91213 time= 0.09624\n",
      "Epoch: 0193 train_loss= 0.43770 train_acc= 0.55001 val_roc= 0.91044 val_ap= 0.91559 time= 0.10642\n",
      "Epoch: 0194 train_loss= 0.43723 train_acc= 0.55088 val_roc= 0.91671 val_ap= 0.92178 time= 0.09592\n",
      "Epoch: 0195 train_loss= 0.43712 train_acc= 0.55200 val_roc= 0.91472 val_ap= 0.92057 time= 0.10404\n",
      "Epoch: 0196 train_loss= 0.43737 train_acc= 0.55072 val_roc= 0.91527 val_ap= 0.92006 time= 0.09095\n",
      "Epoch: 0197 train_loss= 0.43699 train_acc= 0.55157 val_roc= 0.92145 val_ap= 0.92632 time= 0.09259\n",
      "Epoch: 0198 train_loss= 0.43714 train_acc= 0.55082 val_roc= 0.91693 val_ap= 0.92173 time= 0.09309\n",
      "Epoch: 0199 train_loss= 0.43687 train_acc= 0.55122 val_roc= 0.91276 val_ap= 0.91750 time= 0.09528\n",
      "Epoch: 0200 train_loss= 0.43722 train_acc= 0.55098 val_roc= 0.90763 val_ap= 0.91672 time= 0.09979\n",
      "End of training! test_roc= 0.91359 test_ap= 0.91930\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    t = time.time()\n",
    "\n",
    "    A_pred = model(features)\n",
    "    optimizer.zero_grad()\n",
    "    loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight = weight_tensor)\n",
    "    kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(A_pred,adj_label)\n",
    "\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, A_pred)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_roc=\", \"{:.5f}\".format(val_roc),\n",
    "          \"val_ap=\", \"{:.5f}\".format(val_ap),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, A_pred)\n",
    "print(\"End of training!\", \"test_roc=\", \"{:.5f}\".format(test_roc),\n",
    "      \"test_ap=\", \"{:.5f}\".format(test_ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c800b1",
   "metadata": {},
   "source": [
    "## Vamos a explorar lo que resultó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ced5a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.encode(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d38039",
   "metadata": {},
   "source": [
    "## Observemos que el VGAE no es probabilístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_pred = torch.sigmoid(torch.matmul(Z,Z.t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05dd9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ce15c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340dc34",
   "metadata": {},
   "source": [
    "## La A reconstruida la voy a pasar a densa para hacer la diferencia con la entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0aa2e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_A = torch.tensor(A.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "294e79ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 2708])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90c78875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3614781.2500, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l1_diff = torch.norm(A_pred - dense_A, p=1)\n",
    "print(l1_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d23073b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9335, 0.5928, 0.7425,  ..., 0.5674, 0.9246, 0.8510],\n",
       "        [0.5928, 0.9691, 0.9287,  ..., 0.2527, 0.7264, 0.5822],\n",
       "        [0.7425, 0.9287, 0.9687,  ..., 0.4744, 0.9346, 0.8611],\n",
       "        ...,\n",
       "        [0.5674, 0.2527, 0.4744,  ..., 0.9393, 0.4420, 0.5902],\n",
       "        [0.9246, 0.7264, 0.9346,  ..., 0.4420, 0.9897, 0.9622],\n",
       "        [0.8510, 0.5822, 0.8611,  ..., 0.5902, 0.9622, 0.9553]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2155a3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1],\n",
       "        [0, 0, 0,  ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fccdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
