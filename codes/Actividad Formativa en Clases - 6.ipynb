{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d67e24-ad70-4e3d-ad62-98629137df29",
   "metadata": {},
   "source": [
    "## Actividad en clase\n",
    "\n",
    "Vamos a usar R-GCN  y GAT para trabajar en esta actividad.\n",
    "\n",
    "- Lea el dataset CiteseerGraphDataset de dgl.\n",
    "- Usando una red GAT, determine cuantos cabezales requiere para obtener su mejor desempeño en accuracy.\n",
    "- Lea el dataset AMDataset de dgl.\n",
    "- Usando una red R-GCN, determine el desempeño de la red sobre un máximo de 10 epochs.\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que las L otorgan un bono en la nota final de la asignatura.\n",
    "\n",
    "***Tiene hasta el final de la clase.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40373cc1-8fa6-4765-8b2b-53c40fa62c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24416bf2-efa0-42fa-9ea3-3090cad01029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f2f0bc-13c7-40a6-acc2-80891e4c940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GATConv\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain(\"relu\")\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        z2 = torch.cat([edges.src[\"z\"], edges.dst[\"z\"]], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {\"e\": F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        return {\"z\": edges.src[\"z\"], \"e\": edges.data[\"e\"]}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox[\"e\"], dim=1)\n",
    "        h = torch.sum(alpha * nodes.mailbox[\"z\"], dim=1)\n",
    "        return {\"h\": h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata[\"z\"] = z\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006aa0de-9cf3-4a3c-9c8e-f510c4adf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge=\"cat\"):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == \"cat\":\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e827e1e-eb25-4430-bcb6-50b1f3ec374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834c9448-521d-4194-bad3-ce91b1840edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import CiteseerGraphDataset\n",
    "\n",
    "dataset = CiteseerGraphDataset()\n",
    "\n",
    "g = dataset[0]\n",
    "\n",
    "train_mask = torch.BoolTensor(g.ndata['train_mask'])\n",
    "test_mask = torch.BoolTensor(g.ndata['test_mask'])\n",
    "features = g.ndata[\"feat\"] \n",
    "labels = g.ndata[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ad4b5d-2aca-45d8-a440-e627de8fb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a451fc89-0db5-47c2-ba4b-f0252f4df1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.7918 | Test Acc 0.1560 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.7896 | Test Acc 0.2010 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.7873 | Test Acc 0.2460 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.7851 | Test Acc 0.2920 | Time(s) 0.0667\n",
      "Epoch 00004 | Loss 1.7829 | Test Acc 0.3470 | Time(s) 0.0828\n",
      "Epoch 00005 | Loss 1.7806 | Test Acc 0.3790 | Time(s) 0.0799\n",
      "Epoch 00006 | Loss 1.7784 | Test Acc 0.4180 | Time(s) 0.0763\n",
      "Epoch 00007 | Loss 1.7761 | Test Acc 0.4470 | Time(s) 0.0744\n",
      "Epoch 00008 | Loss 1.7739 | Test Acc 0.4650 | Time(s) 0.0731\n",
      "Epoch 00009 | Loss 1.7716 | Test Acc 0.4960 | Time(s) 0.0721\n",
      "Epoch 00010 | Loss 1.7693 | Test Acc 0.5100 | Time(s) 0.0712\n",
      "Epoch 00011 | Loss 1.7670 | Test Acc 0.5190 | Time(s) 0.0707\n",
      "Epoch 00012 | Loss 1.7647 | Test Acc 0.5240 | Time(s) 0.0703\n",
      "Epoch 00013 | Loss 1.7624 | Test Acc 0.5310 | Time(s) 0.0699\n",
      "Epoch 00014 | Loss 1.7601 | Test Acc 0.5420 | Time(s) 0.0699\n",
      "Epoch 00015 | Loss 1.7578 | Test Acc 0.5470 | Time(s) 0.0699\n",
      "Epoch 00016 | Loss 1.7554 | Test Acc 0.5510 | Time(s) 0.0706\n",
      "Epoch 00017 | Loss 1.7531 | Test Acc 0.5630 | Time(s) 0.0702\n",
      "Epoch 00018 | Loss 1.7507 | Test Acc 0.5710 | Time(s) 0.0701\n",
      "Epoch 00019 | Loss 1.7483 | Test Acc 0.5840 | Time(s) 0.0696\n",
      "Epoch 00020 | Loss 1.7459 | Test Acc 0.5870 | Time(s) 0.0691\n",
      "Epoch 00021 | Loss 1.7435 | Test Acc 0.5890 | Time(s) 0.0696\n",
      "Epoch 00022 | Loss 1.7411 | Test Acc 0.5930 | Time(s) 0.0694\n",
      "Epoch 00023 | Loss 1.7387 | Test Acc 0.5980 | Time(s) 0.0693\n",
      "Epoch 00024 | Loss 1.7362 | Test Acc 0.6000 | Time(s) 0.0692\n",
      "Epoch 00025 | Loss 1.7337 | Test Acc 0.6030 | Time(s) 0.0689\n",
      "Epoch 00026 | Loss 1.7313 | Test Acc 0.6060 | Time(s) 0.0690\n",
      "Epoch 00027 | Loss 1.7288 | Test Acc 0.6080 | Time(s) 0.0690\n",
      "Epoch 00028 | Loss 1.7263 | Test Acc 0.6080 | Time(s) 0.0687\n",
      "Epoch 00029 | Loss 1.7237 | Test Acc 0.6150 | Time(s) 0.0684\n",
      "Epoch 00030 | Loss 1.7212 | Test Acc 0.6180 | Time(s) 0.0688\n",
      "Epoch 00031 | Loss 1.7186 | Test Acc 0.6210 | Time(s) 0.0688\n",
      "Epoch 00032 | Loss 1.7161 | Test Acc 0.6210 | Time(s) 0.0686\n",
      "Epoch 00033 | Loss 1.7135 | Test Acc 0.6220 | Time(s) 0.0683\n",
      "Epoch 00034 | Loss 1.7109 | Test Acc 0.6230 | Time(s) 0.0687\n",
      "Epoch 00035 | Loss 1.7083 | Test Acc 0.6220 | Time(s) 0.0684\n",
      "Epoch 00036 | Loss 1.7056 | Test Acc 0.6210 | Time(s) 0.0685\n",
      "Epoch 00037 | Loss 1.7030 | Test Acc 0.6230 | Time(s) 0.0684\n",
      "Epoch 00038 | Loss 1.7003 | Test Acc 0.6230 | Time(s) 0.0681\n",
      "Epoch 00039 | Loss 1.6976 | Test Acc 0.6220 | Time(s) 0.0691\n",
      "Epoch 00040 | Loss 1.6949 | Test Acc 0.6220 | Time(s) 0.0694\n",
      "Epoch 00041 | Loss 1.6922 | Test Acc 0.6230 | Time(s) 0.0693\n",
      "Epoch 00042 | Loss 1.6894 | Test Acc 0.6260 | Time(s) 0.0691\n",
      "Epoch 00043 | Loss 1.6867 | Test Acc 0.6270 | Time(s) 0.0689\n",
      "Epoch 00044 | Loss 1.6839 | Test Acc 0.6270 | Time(s) 0.0690\n",
      "Epoch 00045 | Loss 1.6811 | Test Acc 0.6260 | Time(s) 0.0689\n",
      "Epoch 00046 | Loss 1.6783 | Test Acc 0.6270 | Time(s) 0.0688\n",
      "Epoch 00047 | Loss 1.6755 | Test Acc 0.6270 | Time(s) 0.0688\n",
      "Epoch 00048 | Loss 1.6726 | Test Acc 0.6270 | Time(s) 0.0686\n",
      "Epoch 00049 | Loss 1.6697 | Test Acc 0.6270 | Time(s) 0.0686\n",
      "Epoch 00050 | Loss 1.6669 | Test Acc 0.6290 | Time(s) 0.0684\n",
      "Epoch 00051 | Loss 1.6640 | Test Acc 0.6280 | Time(s) 0.0683\n",
      "Epoch 00052 | Loss 1.6610 | Test Acc 0.6290 | Time(s) 0.0684\n",
      "Epoch 00053 | Loss 1.6581 | Test Acc 0.6300 | Time(s) 0.0683\n",
      "Epoch 00054 | Loss 1.6551 | Test Acc 0.6310 | Time(s) 0.0683\n",
      "Epoch 00055 | Loss 1.6522 | Test Acc 0.6300 | Time(s) 0.0683\n",
      "Epoch 00056 | Loss 1.6492 | Test Acc 0.6300 | Time(s) 0.0681\n",
      "Epoch 00057 | Loss 1.6461 | Test Acc 0.6290 | Time(s) 0.0681\n",
      "Epoch 00058 | Loss 1.6431 | Test Acc 0.6300 | Time(s) 0.0680\n",
      "Epoch 00059 | Loss 1.6401 | Test Acc 0.6300 | Time(s) 0.0681\n",
      "Epoch 00060 | Loss 1.6370 | Test Acc 0.6300 | Time(s) 0.0685\n",
      "Epoch 00061 | Loss 1.6339 | Test Acc 0.6300 | Time(s) 0.0684\n",
      "Epoch 00062 | Loss 1.6308 | Test Acc 0.6300 | Time(s) 0.0685\n",
      "Epoch 00063 | Loss 1.6277 | Test Acc 0.6300 | Time(s) 0.0686\n",
      "Epoch 00064 | Loss 1.6245 | Test Acc 0.6290 | Time(s) 0.0685\n",
      "Epoch 00065 | Loss 1.6213 | Test Acc 0.6290 | Time(s) 0.0685\n",
      "Epoch 00066 | Loss 1.6182 | Test Acc 0.6310 | Time(s) 0.0684\n",
      "Epoch 00067 | Loss 1.6150 | Test Acc 0.6310 | Time(s) 0.0685\n",
      "Epoch 00068 | Loss 1.6117 | Test Acc 0.6320 | Time(s) 0.0684\n",
      "Epoch 00069 | Loss 1.6085 | Test Acc 0.6310 | Time(s) 0.0685\n",
      "Epoch 00070 | Loss 1.6052 | Test Acc 0.6310 | Time(s) 0.0684\n",
      "Epoch 00071 | Loss 1.6020 | Test Acc 0.6330 | Time(s) 0.0684\n",
      "Epoch 00072 | Loss 1.5987 | Test Acc 0.6330 | Time(s) 0.0684\n",
      "Epoch 00073 | Loss 1.5953 | Test Acc 0.6320 | Time(s) 0.0685\n",
      "Epoch 00074 | Loss 1.5920 | Test Acc 0.6330 | Time(s) 0.0685\n",
      "Epoch 00075 | Loss 1.5887 | Test Acc 0.6350 | Time(s) 0.0687\n",
      "Epoch 00076 | Loss 1.5853 | Test Acc 0.6360 | Time(s) 0.0687\n",
      "Epoch 00077 | Loss 1.5819 | Test Acc 0.6350 | Time(s) 0.0686\n",
      "Epoch 00078 | Loss 1.5785 | Test Acc 0.6350 | Time(s) 0.0686\n",
      "Epoch 00079 | Loss 1.5751 | Test Acc 0.6350 | Time(s) 0.0690\n",
      "Epoch 00080 | Loss 1.5716 | Test Acc 0.6350 | Time(s) 0.0689\n",
      "Epoch 00081 | Loss 1.5682 | Test Acc 0.6350 | Time(s) 0.0689\n",
      "Epoch 00082 | Loss 1.5647 | Test Acc 0.6350 | Time(s) 0.0689\n",
      "Epoch 00083 | Loss 1.5612 | Test Acc 0.6360 | Time(s) 0.0689\n",
      "Epoch 00084 | Loss 1.5577 | Test Acc 0.6370 | Time(s) 0.0688\n",
      "Epoch 00085 | Loss 1.5541 | Test Acc 0.6390 | Time(s) 0.0688\n",
      "Epoch 00086 | Loss 1.5506 | Test Acc 0.6400 | Time(s) 0.0688\n",
      "Epoch 00087 | Loss 1.5470 | Test Acc 0.6370 | Time(s) 0.0688\n",
      "Epoch 00088 | Loss 1.5434 | Test Acc 0.6370 | Time(s) 0.0688\n",
      "Epoch 00089 | Loss 1.5398 | Test Acc 0.6370 | Time(s) 0.0688\n",
      "Epoch 00090 | Loss 1.5362 | Test Acc 0.6370 | Time(s) 0.0689\n",
      "Epoch 00091 | Loss 1.5325 | Test Acc 0.6370 | Time(s) 0.0689\n",
      "Epoch 00092 | Loss 1.5288 | Test Acc 0.6360 | Time(s) 0.0688\n",
      "Epoch 00093 | Loss 1.5252 | Test Acc 0.6370 | Time(s) 0.0688\n",
      "Epoch 00094 | Loss 1.5215 | Test Acc 0.6370 | Time(s) 0.0689\n",
      "Epoch 00095 | Loss 1.5177 | Test Acc 0.6370 | Time(s) 0.0689\n",
      "Epoch 00096 | Loss 1.5140 | Test Acc 0.6370 | Time(s) 0.0689\n",
      "Epoch 00097 | Loss 1.5103 | Test Acc 0.6370 | Time(s) 0.0689\n",
      "Epoch 00098 | Loss 1.5065 | Test Acc 0.6370 | Time(s) 0.0688\n",
      "Epoch 00099 | Loss 1.5027 | Test Acc 0.6370 | Time(s) 0.0688\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "net = GAT(g, in_dim=features.size()[1], hidden_dim=8, out_dim=6, num_heads=2)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "dur = []\n",
    "for epoch in range(100):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    logits = net(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\n",
    "        \"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f96519f8-8e25-48ee-895e-fdf9d4b0d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        num_rels,\n",
    "        num_bases=-1,\n",
    "        bias=None,\n",
    "        activation=None,\n",
    "        is_input_layer=False,\n",
    "    ):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        \n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(self.num_bases, self.in_feat, self.out_feat)\n",
    "        )\n",
    "        if self.num_bases < self.num_rels:\n",
    "            \n",
    "            self.w_comp = nn.Parameter(\n",
    "                torch.Tensor(self.num_rels, self.num_bases)\n",
    "            )\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(\n",
    "            self.weight, gain=nn.init.calculate_gain(\"relu\")\n",
    "        )\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(\n",
    "                self.w_comp, gain=nn.init.calculate_gain(\"relu\")\n",
    "            )\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(\n",
    "                self.bias, gain=nn.init.calculate_gain(\"relu\")\n",
    "            )\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:\n",
    "            weight = self.weight.view(\n",
    "                self.in_feat, self.num_bases, self.out_feat\n",
    "            )\n",
    "            weight = torch.matmul(self.w_comp, weight).view(\n",
    "                self.num_rels, self.in_feat, self.out_feat\n",
    "            )\n",
    "        else:\n",
    "            weight = self.weight\n",
    "        if self.is_input_layer:\n",
    "\n",
    "            def message_func(edges):\n",
    "                embed = weight.view(-1, self.out_feat)\n",
    "                index = edges.data[dgl.ETYPE] * self.in_feat + edges.src[\"id\"]\n",
    "                return {\"msg\": embed[index] * edges.data[\"norm\"]}\n",
    "\n",
    "        else:\n",
    "\n",
    "            def message_func(edges):\n",
    "                w = weight[edges.data[dgl.ETYPE]]\n",
    "                msg = torch.bmm(edges.src[\"h\"].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data[\"norm\"]\n",
    "                return {\"msg\": msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data[\"h\"]\n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {\"h\": h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg=\"msg\", out=\"h\"), apply_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed4ec71-b6be-46e5-9dc5-8687555fd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes,\n",
    "        h_dim,\n",
    "        out_dim,\n",
    "        num_rels,\n",
    "        num_bases=-1,\n",
    "        num_hidden_layers=1,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.num_nodes,\n",
    "            self.h_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=F.relu,\n",
    "            is_input_layer=True,\n",
    "        )\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.h_dim,\n",
    "            self.h_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=F.relu,\n",
    "        )\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.h_dim,\n",
    "            self.out_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=partial(F.softmax, dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.features is not None:\n",
    "            g.ndata[\"id\"] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop(\"h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "324b29b2-2cd9-46fe-89b4-ce2c202912d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import AMDataset\n",
    "\n",
    "dataset = AMDataset()\n",
    "g = dataset[0]\n",
    "\n",
    "category = dataset.predict_category\n",
    "train_mask = g.nodes[category].data.pop(\"train_mask\")\n",
    "test_mask = g.nodes[category].data.pop(\"test_mask\")\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "labels = g.nodes[category].data.pop(\"label\")\n",
    "num_rels = len(g.canonical_etypes)\n",
    "num_classes = dataset.num_classes\n",
    "# normalization factor\n",
    "for cetype in g.canonical_etypes:\n",
    "    g.edges[cetype].data[\"norm\"] = dgl.norm_by_dst(g, cetype).unsqueeze(1)\n",
    "category_id = g.ntypes.index(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c6477a2-d8f7-44e8-b56e-1ac4dbfb4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=1885136, num_edges=5668682,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'norm': Scheme(shape=(1,), dtype=torch.float32), '_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "# configurations\n",
    "n_hidden = 16  # number of hidden units\n",
    "n_bases = -1  # use number of relations as number of bases\n",
    "n_hidden_layers = 0  # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 10  # epochs to train\n",
    "lr = 0.01  # learning rate\n",
    "l2norm = 0  # L2 norm coefficient\n",
    "\n",
    "g = dgl.to_homogeneous(g, edata=[\"norm\"])\n",
    "node_ids = torch.arange(g.num_nodes())\n",
    "target_idx = node_ids[g.ndata[dgl.NTYPE] == category_id]\n",
    "\n",
    "model = Model(\n",
    "    g.num_nodes(),\n",
    "    n_hidden,\n",
    "    num_classes,\n",
    "    num_rels,\n",
    "    num_bases=n_bases,\n",
    "    num_hidden_layers=n_hidden_layers,\n",
    ")\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929279cf-d6df-46df-884a-2be2094cf8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.0810 | Train Loss: 2.3979 | Validation Accuracy: 0.0556 | Validation loss: 2.3979\n",
      "Epoch 00001 | Train Accuracy: 0.7706 | Train Loss: 2.3905 | Validation Accuracy: 0.5253 | Validation loss: 2.3937\n",
      "Epoch 00002 | Train Accuracy: 0.7793 | Train Loss: 2.3753 | Validation Accuracy: 0.5505 | Validation loss: 2.3852\n",
      "Epoch 00003 | Train Accuracy: 0.7818 | Train Loss: 2.3497 | Validation Accuracy: 0.5606 | Validation loss: 2.3715\n",
      "Epoch 00004 | Train Accuracy: 0.7693 | Train Loss: 2.3092 | Validation Accuracy: 0.5505 | Validation loss: 2.3508\n",
      "Epoch 00005 | Train Accuracy: 0.7606 | Train Loss: 2.2503 | Validation Accuracy: 0.5606 | Validation loss: 2.3209\n",
      "Epoch 00006 | Train Accuracy: 0.7544 | Train Loss: 2.1760 | Validation Accuracy: 0.5606 | Validation loss: 2.2802\n",
      "Epoch 00007 | Train Accuracy: 0.7494 | Train Loss: 2.0966 | Validation Accuracy: 0.5505 | Validation loss: 2.2292\n",
      "Epoch 00008 | Train Accuracy: 0.7531 | Train Loss: 2.0243 | Validation Accuracy: 0.5556 | Validation loss: 2.1734\n",
      "Epoch 00009 | Train Accuracy: 0.7569 | Train Loss: 1.9633 | Validation Accuracy: 0.5606 | Validation loss: 2.1219\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    logits = logits[target_idx]\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "    val_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx])\n",
    "    val_acc = val_acc.item() / len(test_idx)\n",
    "    print(\n",
    "        \"Epoch {:05d} | \".format(epoch)\n",
    "        + \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "            train_acc, loss.item()\n",
    "        )\n",
    "        + \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "            val_acc, val_loss.item()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3bdd6c-3c59-422b-affb-9c0311fb560b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
