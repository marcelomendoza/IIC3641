{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d67e24-ad70-4e3d-ad62-98629137df29",
   "metadata": {},
   "source": [
    "## Actividad en clase\n",
    "\n",
    "Vamos a usar el Graph Transformer para trabajar en esta actividad.\n",
    "\n",
    "- Corra el graph transformer y vea que puede reproducir el ejemplo de la clase.\n",
    "- Use el dataset ppbr_az de que viene en ADMET_Group. \n",
    "- Reentrene el graph transformer. Pruebe variantes con distinto número de cabezales y capas para ver el efecto que esto tiene en el MAE. Haga 3 variantes, al menos, y corra 100 epochs.\n",
    "- Vea la última lámina de la clase y dígame si le ganó al paper.\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que las L otorgan un bono en la nota final de la asignatura.\n",
    "\n",
    "***Tiene hasta el final de la clase.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3bdd6c-3c59-422b-affb-9c0311fb560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelo/.local/lib/python3.8/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/marcelo/.local/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops10zeros_like4callERKNS_6TensorEN3c108optionalINS5_10ScalarTypeEEENS6_INS5_6LayoutEEENS6_INS5_6DeviceEEENS6_IbEENS6_INS5_12MemoryFormatEEE\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/marcelo/.local/lib/python3.8/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/marcelo/.local/lib/python3.8/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/marcelo/.local/lib/python3.8/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/marcelo/.local/lib/python3.8/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/marcelo/.local/lib/python3.8/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from gt_pyg.gt_pyg.nn.gt_conv import GTConv\n",
    "\n",
    "# Standard\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import RDLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanAbsoluteError\n",
    "\n",
    "import gt_pyg.gt_pyg\n",
    "\n",
    "from gt_pyg.gt_pyg.data.utils import (\n",
    "    get_tensor_data, \n",
    "    get_node_dim, \n",
    "    get_edge_dim, \n",
    "    get_train_valid_test_data\n",
    ")\n",
    "from gt_pyg.gt_pyg.nn.model import GraphTransformerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a892aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available endpoints:\n",
      "\n",
      "1. caco2_wang\n",
      "2. hia_hou\n",
      "3. pgp_broccatelli\n",
      "4. bioavailability_ma\n",
      "5. lipophilicity_astrazeneca\n",
      "6. solubility_aqsoldb\n",
      "7. bbb_martins\n",
      "8. ppbr_az\n",
      "9. vdss_lombardo\n",
      "10. cyp2d6_veith\n",
      "11. cyp3a4_veith\n",
      "12. cyp2c9_veith\n",
      "13. cyp2d6_substrate_carbonmangels\n",
      "14. cyp3a4_substrate_carbonmangels\n",
      "15. cyp2c9_substrate_carbonmangels\n",
      "16. half_life_obach\n",
      "17. clearance_microsome_az\n",
      "18. clearance_hepatocyte_az\n",
      "19. herg\n",
      "20. ames\n",
      "21. dili\n",
      "22. ld50_zhu\n"
     ]
    }
   ],
   "source": [
    "# Turn off majority of RDKit warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "# Setup the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from tdc import utils\n",
    "names = utils.retrieve_benchmark_names('ADMET_Group')\n",
    "output = \"\\n\".join([f\"{index}. {name}\" for index, name in enumerate(names, start=1)])\n",
    "print(\"Available endpoints:\\n\\n\" + output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c66635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f82e223bc15427386c6c5b10bb63267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270f522dd33f4e7882886815de262849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736cb8cc2cf243a09584ebb25dd8e907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b811374092646188521565b163b3c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd26c2b7a8454f01be45f4048f10e01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1130\n",
      "Number of validation examples: 161\n",
      "Number of test examples: 323\n"
     ]
    }
   ],
   "source": [
    "PE_DIM = 6 \n",
    "(tr, va, te) = get_train_valid_test_data('ppbr_az', min_num_atoms=0)\n",
    "tr_dataset = get_tensor_data(tr.Drug.to_list(), tr.Y.to_list(), pe_dim=PE_DIM)\n",
    "va_dataset = get_tensor_data(va.Drug.to_list(), va.Y.to_list(), pe_dim=PE_DIM)\n",
    "te_dataset = get_tensor_data(te.Drug.to_list(), te.Y.to_list(), pe_dim=PE_DIM)\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "print(f'Number of training examples: {len(tr_dataset)}')\n",
    "print(f'Number of validation examples: {len(va_dataset)}')\n",
    "print(f'Number of test examples: {len(te_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=64)\n",
    "val_loader = DataLoader(va_dataset, batch_size=512)\n",
    "test_loader = DataLoader(te_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c619024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node dim: 76\n",
      "Edge dim: 10\n"
     ]
    }
   ],
   "source": [
    "print(f'Node dim: {NODE_DIM}')\n",
    "print(f'Edge dim: {EDGE_DIM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa988b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "    train_mae = MeanAbsoluteError().to(device)\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # randomly flip sign of eigenvectors\n",
    "        \n",
    "        #device = data.pe.device  # Get the device of data.pe\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM), device=device).float() - 1.0)\n",
    "        #batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch, zero_var=False)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_mae.update(out.squeeze(), data.y)\n",
    "\n",
    "    return train_mae.compute()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    test_mae = MeanAbsoluteError().to(device)\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM), device=device).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        \n",
    "        test_mae.update(out.squeeze(), data.y)\n",
    "        \n",
    "    return test_mae.compute()\n",
    "\n",
    "train_loss = nn.L1Loss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011bbf1",
   "metadata": {},
   "source": [
    "## Modelo 1, el mismo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9299d60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GraphTransformerNet(\n",
      "    (node_emb): Linear(in_features=76, out_features=128, bias=False)\n",
      "    (edge_emb): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (pe_emb): Linear(in_features=6, out_features=128, bias=False)\n",
      "    (gt_layers): ModuleList(\n",
      "      (0-3): 4 x GTConv(128, 128, heads=8, aggrs: sum, qkv_bias: False, gate: False)\n",
      "    )\n",
      "    (global_pool): MultiAggregation([\n",
      "      SumAggregation(),\n",
      "    ], mode=cat)\n",
      "    (mu_mlp): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (log_var_mlp): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 708 k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uncaught exception in compile_worker subprocess\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/compile_worker/__main__.py\", line 38, in main\n",
      "    pre_fork_setup()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/async_compile.py\", line 62, in pre_fork_setup\n",
      "    from triton.compiler.compiler import triton_key\n",
      "ImportError: cannot import name 'triton_key' from 'triton.compiler.compiler' (/usr/local/lib/python3.8/dist-packages/triton/compiler/compiler.py)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric import *\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=4, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='bn',\n",
    "                            gate=False,\n",
    "                            qkv_bias=False,\n",
    "                            gt_aggregators=['sum'],\n",
    "                            aggregators=['sum'],\n",
    "                            dropout=0.1,\n",
    "                            act='relu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef990df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 10.1665, Val: 12.6608, Test: 13.6024\n",
      "Epoch: 02, Loss: 10.0647, Val: 9.9974, Test: 10.4031\n",
      "Epoch: 03, Loss: 9.9231, Val: 10.6948, Test: 10.2096\n",
      "Epoch: 04, Loss: 10.0852, Val: 22.0640, Test: 21.2211\n",
      "Epoch: 05, Loss: 10.1501, Val: 13.9527, Test: 13.0613\n",
      "Epoch: 06, Loss: 10.3019, Val: 9.8290, Test: 9.7904\n",
      "Epoch: 07, Loss: 10.4457, Val: 11.0555, Test: 10.3235\n",
      "Epoch: 08, Loss: 9.8910, Val: 11.5018, Test: 11.9727\n",
      "Epoch: 09, Loss: 9.7831, Val: 10.0029, Test: 9.8427\n",
      "Epoch: 10, Loss: 9.7450, Val: 8.9749, Test: 9.7999\n",
      "Epoch: 11, Loss: 9.5890, Val: 10.3776, Test: 9.8586\n",
      "Epoch: 12, Loss: 9.5558, Val: 9.6941, Test: 10.4032\n",
      "Epoch: 13, Loss: 9.4649, Val: 10.4144, Test: 11.3883\n",
      "Epoch: 14, Loss: 9.3803, Val: 9.6984, Test: 9.8036\n",
      "Epoch: 15, Loss: 9.4742, Val: 8.7416, Test: 9.3590\n",
      "Epoch: 16, Loss: 9.4356, Val: 12.3145, Test: 12.6562\n",
      "Epoch: 17, Loss: 9.4620, Val: 8.9924, Test: 9.3180\n",
      "Epoch: 18, Loss: 9.2845, Val: 10.7053, Test: 10.4935\n",
      "Epoch: 19, Loss: 9.1825, Val: 9.4052, Test: 9.5097\n",
      "Epoch: 20, Loss: 9.1563, Val: 8.6752, Test: 9.3619\n",
      "Epoch: 21, Loss: 8.9094, Val: 11.1902, Test: 11.6943\n",
      "Epoch: 22, Loss: 9.0412, Val: 8.6719, Test: 9.3818\n",
      "Epoch: 23, Loss: 9.0570, Val: 10.3458, Test: 11.0335\n",
      "Epoch: 24, Loss: 9.0550, Val: 9.4602, Test: 9.8288\n",
      "Epoch: 25, Loss: 8.9910, Val: 9.4002, Test: 9.6516\n",
      "Epoch: 26, Loss: 8.8981, Val: 9.9674, Test: 9.3056\n",
      "Epoch: 27, Loss: 9.0385, Val: 9.9152, Test: 10.0579\n",
      "Epoch: 28, Loss: 8.8197, Val: 9.9308, Test: 9.6245\n",
      "Epoch: 29, Loss: 8.9042, Val: 9.7757, Test: 10.0397\n",
      "Epoch: 30, Loss: 9.0279, Val: 8.9594, Test: 9.0701\n",
      "Epoch: 31, Loss: 8.8020, Val: 11.0846, Test: 10.1913\n",
      "Epoch: 32, Loss: 8.7440, Val: 12.8021, Test: 13.8136\n",
      "Epoch: 33, Loss: 8.5848, Val: 12.7193, Test: 10.9776\n",
      "Epoch: 34, Loss: 8.6317, Val: 9.0825, Test: 9.6437\n",
      "Epoch: 35, Loss: 8.4185, Val: 9.0690, Test: 9.1437\n",
      "Epoch: 36, Loss: 8.6000, Val: 9.1547, Test: 9.4213\n",
      "Epoch: 37, Loss: 8.6342, Val: 9.2985, Test: 9.9169\n",
      "Epoch: 38, Loss: 8.4164, Val: 10.2165, Test: 9.6921\n",
      "Epoch: 39, Loss: 8.4137, Val: 9.0975, Test: 9.2276\n",
      "Epoch: 40, Loss: 8.3598, Val: 9.4803, Test: 9.6450\n",
      "Epoch: 41, Loss: 8.4396, Val: 8.8683, Test: 9.2176\n",
      "Epoch: 42, Loss: 8.2731, Val: 8.8032, Test: 9.4045\n",
      "Epoch: 43, Loss: 8.4640, Val: 9.0652, Test: 9.0724\n",
      "Epoch: 44, Loss: 8.3355, Val: 9.4592, Test: 9.6043\n",
      "Epoch: 45, Loss: 8.2330, Val: 8.6744, Test: 9.1462\n",
      "Epoch: 46, Loss: 8.2838, Val: 9.7895, Test: 9.4728\n",
      "Epoch: 47, Loss: 8.2783, Val: 8.8951, Test: 9.3757\n",
      "Epoch: 48, Loss: 8.2830, Val: 8.7023, Test: 9.2537\n",
      "Epoch: 49, Loss: 8.1818, Val: 8.7708, Test: 9.1114\n",
      "Epoch: 50, Loss: 8.1986, Val: 8.6946, Test: 9.4169\n",
      "Epoch: 51, Loss: 8.2022, Val: 8.7829, Test: 8.9585\n",
      "Epoch: 52, Loss: 8.1015, Val: 8.6395, Test: 8.9199\n",
      "Epoch: 53, Loss: 8.2348, Val: 8.5438, Test: 9.1688\n",
      "Epoch: 54, Loss: 8.1565, Val: 8.4717, Test: 8.9974\n",
      "Epoch: 55, Loss: 8.1987, Val: 8.5966, Test: 8.9403\n",
      "Epoch: 56, Loss: 8.2860, Val: 8.7483, Test: 8.9568\n",
      "Epoch: 57, Loss: 8.1572, Val: 9.4525, Test: 9.9013\n",
      "Epoch: 58, Loss: 8.1577, Val: 8.5215, Test: 9.0118\n",
      "Epoch: 59, Loss: 8.1648, Val: 8.4583, Test: 8.9724\n",
      "Epoch: 60, Loss: 8.0033, Val: 8.8418, Test: 9.1359\n",
      "Epoch: 61, Loss: 8.1316, Val: 9.2653, Test: 9.7633\n",
      "Epoch: 62, Loss: 8.2254, Val: 8.6373, Test: 9.2198\n",
      "Epoch: 63, Loss: 8.2022, Val: 8.7707, Test: 9.4224\n",
      "Epoch: 64, Loss: 8.0055, Val: 8.6781, Test: 9.1703\n",
      "Epoch: 65, Loss: 8.0362, Val: 9.0394, Test: 9.2506\n",
      "Epoch: 66, Loss: 8.0749, Val: 8.4426, Test: 8.9841\n",
      "Epoch: 67, Loss: 8.0444, Val: 8.5227, Test: 8.9761\n",
      "Epoch: 68, Loss: 8.2144, Val: 8.4234, Test: 8.9730\n",
      "Epoch: 69, Loss: 8.1543, Val: 8.7862, Test: 9.3613\n",
      "Epoch: 70, Loss: 8.0806, Val: 9.1417, Test: 9.0362\n",
      "Epoch: 71, Loss: 7.9473, Val: 8.7048, Test: 9.3314\n",
      "Epoch: 72, Loss: 7.9343, Val: 8.7072, Test: 9.0442\n",
      "Epoch: 73, Loss: 8.0230, Val: 8.8402, Test: 9.2894\n",
      "Epoch: 74, Loss: 8.0547, Val: 8.7961, Test: 9.1694\n",
      "Epoch: 75, Loss: 7.9432, Val: 8.7165, Test: 9.3502\n",
      "Epoch: 76, Loss: 8.0097, Val: 8.6105, Test: 8.9243\n",
      "Epoch: 77, Loss: 7.8950, Val: 8.5071, Test: 8.9836\n",
      "Epoch: 78, Loss: 8.0034, Val: 8.7203, Test: 9.0036\n",
      "Epoch: 79, Loss: 8.0867, Val: 8.4210, Test: 9.0313\n",
      "Epoch: 80, Loss: 7.8758, Val: 8.6307, Test: 9.0636\n",
      "Epoch: 81, Loss: 7.9195, Val: 8.5222, Test: 8.9569\n",
      "Epoch: 82, Loss: 7.9096, Val: 9.4665, Test: 10.1353\n",
      "Epoch: 83, Loss: 7.8320, Val: 10.6391, Test: 10.1406\n",
      "Epoch: 84, Loss: 7.9204, Val: 8.3099, Test: 9.1250\n",
      "Epoch: 85, Loss: 7.9440, Val: 8.8462, Test: 9.3999\n",
      "Epoch: 86, Loss: 7.8967, Val: 9.0987, Test: 8.9461\n",
      "Epoch: 87, Loss: 7.8852, Val: 8.5518, Test: 9.0307\n",
      "Epoch: 88, Loss: 7.8374, Val: 8.9183, Test: 9.3280\n",
      "Epoch: 89, Loss: 7.7254, Val: 8.5286, Test: 9.2769\n",
      "Epoch: 90, Loss: 7.8073, Val: 9.8332, Test: 10.2469\n",
      "Epoch: 91, Loss: 7.8728, Val: 9.4421, Test: 9.0909\n",
      "Epoch: 92, Loss: 7.6886, Val: 11.1496, Test: 11.3530\n",
      "Epoch: 93, Loss: 7.9374, Val: 8.8706, Test: 9.1372\n",
      "Epoch: 94, Loss: 7.7362, Val: 9.2491, Test: 9.2939\n",
      "Epoch: 95, Loss: 7.7882, Val: 8.7876, Test: 9.3232\n",
      "Epoch: 96, Loss: 7.8612, Val: 8.9042, Test: 9.4874\n",
      "Epoch: 97, Loss: 7.7851, Val: 8.5414, Test: 9.1520\n",
      "Epoch: 98, Loss: 7.7682, Val: 8.6098, Test: 9.0176\n",
      "Epoch: 99, Loss: 7.8510, Val: 8.5604, Test: 9.2439\n",
      "Epoch: 100, Loss: 7.7485, Val: 8.8651, Test: 9.5753\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "MAE=9.125029563903809\n",
      "Epoch=84\n"
     ]
    }
   ],
   "source": [
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader)\n",
    "    te_loss = test(test_loader)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16de32",
   "metadata": {},
   "source": [
    "## Modelo 2, bajo los cabezales a la mitad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb9f18c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GraphTransformerNet(\n",
      "    (node_emb): Linear(in_features=76, out_features=128, bias=False)\n",
      "    (edge_emb): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (pe_emb): Linear(in_features=6, out_features=128, bias=False)\n",
      "    (gt_layers): ModuleList(\n",
      "      (0-3): 4 x GTConv(128, 128, heads=4, aggrs: sum, qkv_bias: False, gate: False)\n",
      "    )\n",
      "    (global_pool): MultiAggregation([\n",
      "      SumAggregation(),\n",
      "    ], mode=cat)\n",
      "    (mu_mlp): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (log_var_mlp): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 708 k\n",
      "Epoch: 01, Loss: 56.0192, Val: 91.8635, Test: 86.3211\n",
      "Epoch: 02, Loss: 27.1171, Val: 32.7959, Test: 31.0223\n",
      "Epoch: 03, Loss: 17.0830, Val: 14.0657, Test: 14.7562\n",
      "Epoch: 04, Loss: 15.6536, Val: 23.4293, Test: 23.0924\n",
      "Epoch: 05, Loss: 16.2319, Val: 17.1748, Test: 18.8746\n",
      "Epoch: 06, Loss: 15.2241, Val: 14.1929, Test: 15.8942\n",
      "Epoch: 07, Loss: 14.4395, Val: 14.9197, Test: 14.8869\n",
      "Epoch: 08, Loss: 14.5408, Val: 24.9760, Test: 25.8479\n",
      "Epoch: 09, Loss: 13.2244, Val: 13.9061, Test: 13.2507\n",
      "Epoch: 10, Loss: 13.6249, Val: 24.5664, Test: 22.5958\n",
      "Epoch: 11, Loss: 12.8569, Val: 15.3067, Test: 15.2263\n",
      "Epoch: 12, Loss: 12.2833, Val: 12.8570, Test: 13.8428\n",
      "Epoch: 13, Loss: 12.5931, Val: 13.4305, Test: 14.3633\n",
      "Epoch: 14, Loss: 13.0865, Val: 16.3477, Test: 15.3174\n",
      "Epoch: 15, Loss: 12.7646, Val: 10.6971, Test: 11.6739\n",
      "Epoch: 16, Loss: 12.2031, Val: 39.9562, Test: 40.3178\n",
      "Epoch: 17, Loss: 12.3683, Val: 21.8640, Test: 20.0239\n",
      "Epoch: 18, Loss: 11.9240, Val: 9.2046, Test: 9.6480\n",
      "Epoch: 19, Loss: 11.8516, Val: 12.1309, Test: 11.6965\n",
      "Epoch: 20, Loss: 12.2199, Val: 17.2883, Test: 16.4313\n",
      "Epoch: 21, Loss: 11.7972, Val: 9.8363, Test: 9.9994\n",
      "Epoch: 22, Loss: 11.1927, Val: 15.5826, Test: 15.8154\n",
      "Epoch: 23, Loss: 11.2676, Val: 25.0084, Test: 23.1278\n",
      "Epoch: 24, Loss: 11.1420, Val: 9.6346, Test: 10.4946\n",
      "Epoch: 25, Loss: 11.5808, Val: 11.2289, Test: 11.6406\n",
      "Epoch: 26, Loss: 11.7680, Val: 17.3907, Test: 19.1717\n",
      "Epoch: 27, Loss: 11.2554, Val: 20.4955, Test: 18.9317\n",
      "Epoch: 28, Loss: 11.4790, Val: 16.2153, Test: 15.9286\n",
      "Epoch: 29, Loss: 11.4169, Val: 21.2568, Test: 22.8951\n",
      "Epoch: 30, Loss: 10.8295, Val: 16.7716, Test: 17.6899\n",
      "Epoch: 31, Loss: 10.6306, Val: 30.3844, Test: 30.4920\n",
      "Epoch: 32, Loss: 11.0851, Val: 9.1074, Test: 10.3634\n",
      "Epoch: 33, Loss: 11.6764, Val: 29.3232, Test: 28.0210\n",
      "Epoch: 34, Loss: 11.2888, Val: 11.1239, Test: 11.3672\n",
      "Epoch: 35, Loss: 10.6883, Val: 11.3056, Test: 12.0554\n",
      "Epoch: 36, Loss: 10.3565, Val: 16.4712, Test: 17.3856\n",
      "Epoch: 37, Loss: 10.5451, Val: 11.2371, Test: 11.2597\n",
      "Epoch: 38, Loss: 10.4130, Val: 12.3485, Test: 13.7135\n",
      "Epoch: 39, Loss: 10.3819, Val: 10.2487, Test: 11.1663\n",
      "Epoch: 40, Loss: 10.5929, Val: 11.7104, Test: 13.5283\n",
      "Epoch: 41, Loss: 10.3118, Val: 13.5202, Test: 15.0608\n",
      "Epoch: 42, Loss: 10.3874, Val: 14.7743, Test: 15.3816\n",
      "Epoch: 43, Loss: 10.2359, Val: 12.5687, Test: 12.3017\n",
      "Epoch: 44, Loss: 10.0418, Val: 10.1817, Test: 10.8344\n",
      "Epoch: 45, Loss: 9.5296, Val: 10.0735, Test: 11.0403\n",
      "Epoch: 46, Loss: 9.5209, Val: 9.4142, Test: 10.4552\n",
      "Epoch: 47, Loss: 9.4647, Val: 12.8203, Test: 13.3719\n",
      "Epoch: 48, Loss: 9.4462, Val: 10.0398, Test: 11.3512\n",
      "Epoch: 49, Loss: 9.5794, Val: 8.7366, Test: 9.9276\n",
      "Epoch: 50, Loss: 9.3133, Val: 9.8554, Test: 10.6893\n",
      "Epoch: 51, Loss: 9.4864, Val: 8.9104, Test: 10.1545\n",
      "Epoch: 52, Loss: 9.1871, Val: 8.9961, Test: 10.3624\n",
      "Epoch: 53, Loss: 9.3699, Val: 8.7849, Test: 10.0442\n",
      "Epoch: 54, Loss: 9.2637, Val: 8.8502, Test: 10.0163\n",
      "Epoch: 55, Loss: 9.2569, Val: 11.7219, Test: 11.9841\n",
      "Epoch: 56, Loss: 9.1588, Val: 9.8319, Test: 10.9287\n",
      "Epoch: 57, Loss: 9.3853, Val: 11.6241, Test: 12.4712\n",
      "Epoch: 58, Loss: 9.0317, Val: 14.9057, Test: 15.7529\n",
      "Epoch: 59, Loss: 9.1571, Val: 11.4930, Test: 12.4057\n",
      "Epoch: 60, Loss: 9.0731, Val: 14.6671, Test: 14.7478\n",
      "Epoch: 61, Loss: 8.9416, Val: 8.5130, Test: 9.5098\n",
      "Epoch: 62, Loss: 9.0086, Val: 8.2755, Test: 9.2541\n",
      "Epoch: 63, Loss: 8.7883, Val: 9.7665, Test: 10.7920\n",
      "Epoch: 64, Loss: 8.8458, Val: 11.6902, Test: 12.6451\n",
      "Epoch: 65, Loss: 8.6991, Val: 8.8731, Test: 9.5935\n",
      "Epoch: 66, Loss: 8.9771, Val: 9.4386, Test: 10.3216\n",
      "Epoch: 67, Loss: 8.7705, Val: 8.8253, Test: 9.6862\n",
      "Epoch: 68, Loss: 8.6370, Val: 9.5973, Test: 10.1902\n",
      "Epoch: 69, Loss: 8.7713, Val: 8.6980, Test: 9.4412\n",
      "Epoch: 70, Loss: 8.9557, Val: 9.2968, Test: 10.0036\n",
      "Epoch: 71, Loss: 8.5374, Val: 9.2973, Test: 10.0191\n",
      "Epoch: 72, Loss: 8.4144, Val: 9.5290, Test: 10.1798\n",
      "Epoch: 73, Loss: 8.4848, Val: 8.6107, Test: 9.6478\n",
      "Epoch: 74, Loss: 8.4056, Val: 8.5442, Test: 9.2874\n",
      "Epoch: 75, Loss: 8.3066, Val: 8.7823, Test: 9.7232\n",
      "Epoch: 76, Loss: 8.2794, Val: 8.4539, Test: 9.2065\n",
      "Epoch: 77, Loss: 8.2076, Val: 8.9219, Test: 9.7699\n",
      "Epoch: 78, Loss: 8.2967, Val: 8.6161, Test: 9.7243\n",
      "Epoch: 79, Loss: 8.2296, Val: 8.7418, Test: 9.3792\n",
      "Epoch: 80, Loss: 8.2997, Val: 8.3774, Test: 9.0996\n",
      "Epoch: 81, Loss: 8.3335, Val: 8.6983, Test: 9.7868\n",
      "Epoch: 82, Loss: 8.1279, Val: 8.4001, Test: 9.2785\n",
      "Epoch: 83, Loss: 8.3884, Val: 8.4135, Test: 9.1637\n",
      "Epoch: 84, Loss: 8.1669, Val: 8.4113, Test: 9.3211\n",
      "Epoch: 85, Loss: 8.1411, Val: 8.3156, Test: 9.1461\n",
      "Epoch: 86, Loss: 8.1826, Val: 8.4824, Test: 9.3208\n",
      "Epoch: 87, Loss: 8.1115, Val: 8.3996, Test: 9.2523\n",
      "Epoch: 88, Loss: 8.0679, Val: 8.3949, Test: 9.1739\n",
      "Epoch: 89, Loss: 8.0707, Val: 8.6450, Test: 9.4042\n",
      "Epoch: 90, Loss: 8.0646, Val: 8.4990, Test: 9.1597\n",
      "Epoch: 91, Loss: 7.9983, Val: 8.5641, Test: 9.1869\n",
      "Epoch: 92, Loss: 8.0758, Val: 8.7173, Test: 9.3431\n",
      "Epoch: 93, Loss: 8.0059, Val: 8.4332, Test: 9.1058\n",
      "Epoch: 94, Loss: 8.0156, Val: 8.5099, Test: 9.2791\n",
      "Epoch: 95, Loss: 7.9965, Val: 8.4018, Test: 9.0730\n",
      "Epoch: 96, Loss: 8.0059, Val: 8.4292, Test: 9.1373\n",
      "Epoch: 97, Loss: 8.0817, Val: 8.6882, Test: 9.4517\n",
      "Epoch: 98, Loss: 7.9188, Val: 8.3665, Test: 9.1729\n",
      "Epoch: 99, Loss: 8.0543, Val: 8.4668, Test: 9.2154\n",
      "Epoch: 100, Loss: 7.9283, Val: 8.4369, Test: 9.1288\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "MAE=9.25407886505127\n",
      "Epoch=62\n"
     ]
    }
   ],
   "source": [
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=4, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=4,\n",
    "                            norm='bn',\n",
    "                            gate=False,\n",
    "                            qkv_bias=False,\n",
    "                            gt_aggregators=['sum'],\n",
    "                            aggregators=['sum'],\n",
    "                            dropout=0.1,\n",
    "                            act='relu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "for epoch in range(1, 101):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader)\n",
    "    te_loss = test(test_loader)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3c5f4",
   "metadata": {},
   "source": [
    "## Modelo 3, igual al modelo 1 pero con la mitad de capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b61dcaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GraphTransformerNet(\n",
      "    (node_emb): Linear(in_features=76, out_features=128, bias=False)\n",
      "    (edge_emb): Linear(in_features=10, out_features=128, bias=False)\n",
      "    (pe_emb): Linear(in_features=6, out_features=128, bias=False)\n",
      "    (gt_layers): ModuleList(\n",
      "      (0-1): 2 x GTConv(128, 128, heads=8, aggrs: sum, qkv_bias: False, gate: False)\n",
      "    )\n",
      "    (global_pool): MultiAggregation([\n",
      "      SumAggregation(),\n",
      "    ], mode=cat)\n",
      "    (mu_mlp): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (log_var_mlp): MLP(\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 376 k\n",
      "Epoch: 01, Loss: 58.6166, Val: 82.5431, Test: 68.1310\n",
      "Epoch: 02, Loss: 35.8914, Val: 38.2512, Test: 38.0924\n",
      "Epoch: 03, Loss: 23.7537, Val: 30.4898, Test: 27.8704\n",
      "Epoch: 04, Loss: 18.0348, Val: 14.8769, Test: 15.9482\n",
      "Epoch: 05, Loss: 16.8804, Val: 21.6847, Test: 21.6951\n",
      "Epoch: 06, Loss: 15.5406, Val: 17.1446, Test: 15.9262\n",
      "Epoch: 07, Loss: 15.9008, Val: 13.6828, Test: 14.0246\n",
      "Epoch: 08, Loss: 14.8460, Val: 12.7736, Test: 12.9133\n",
      "Epoch: 09, Loss: 14.6537, Val: 13.1612, Test: 13.4956\n",
      "Epoch: 10, Loss: 14.6679, Val: 15.1963, Test: 13.7632\n",
      "Epoch: 11, Loss: 13.9493, Val: 18.5092, Test: 16.8786\n",
      "Epoch: 12, Loss: 14.3374, Val: 17.5559, Test: 16.3705\n",
      "Epoch: 13, Loss: 13.4592, Val: 13.0220, Test: 14.3508\n",
      "Epoch: 14, Loss: 13.0485, Val: 17.8112, Test: 18.9829\n",
      "Epoch: 15, Loss: 12.9772, Val: 25.4505, Test: 24.5472\n",
      "Epoch: 16, Loss: 12.8341, Val: 26.7629, Test: 25.8374\n",
      "Epoch: 17, Loss: 12.6582, Val: 11.9035, Test: 11.4125\n",
      "Epoch: 18, Loss: 12.3066, Val: 15.0855, Test: 16.0247\n",
      "Epoch: 19, Loss: 12.1793, Val: 20.7376, Test: 19.3963\n",
      "Epoch: 20, Loss: 12.1518, Val: 13.0379, Test: 14.7896\n",
      "Epoch: 21, Loss: 11.9570, Val: 13.2333, Test: 13.1175\n",
      "Epoch: 22, Loss: 12.5566, Val: 19.7660, Test: 18.5281\n",
      "Epoch: 23, Loss: 11.7853, Val: 20.3478, Test: 18.6348\n",
      "Epoch: 24, Loss: 11.5771, Val: 13.8930, Test: 13.3088\n",
      "Epoch: 25, Loss: 11.3464, Val: 12.1932, Test: 12.1376\n",
      "Epoch: 26, Loss: 11.3874, Val: 14.2115, Test: 14.1980\n",
      "Epoch: 27, Loss: 11.4334, Val: 13.0870, Test: 11.9798\n",
      "Epoch: 28, Loss: 11.4697, Val: 26.6534, Test: 26.9090\n",
      "Epoch: 29, Loss: 11.2997, Val: 10.8271, Test: 11.1026\n",
      "Epoch: 30, Loss: 11.3866, Val: 15.0025, Test: 15.5123\n",
      "Epoch: 31, Loss: 11.2021, Val: 12.6963, Test: 13.7286\n",
      "Epoch: 32, Loss: 11.1389, Val: 12.0420, Test: 12.5762\n",
      "Epoch: 33, Loss: 10.7449, Val: 10.9528, Test: 11.6343\n",
      "Epoch: 34, Loss: 10.7012, Val: 18.5623, Test: 18.1611\n",
      "Epoch: 35, Loss: 10.3656, Val: 11.5450, Test: 11.2006\n",
      "Epoch: 36, Loss: 10.4410, Val: 10.0019, Test: 10.6987\n",
      "Epoch: 37, Loss: 10.4816, Val: 9.1876, Test: 9.3108\n",
      "Epoch: 38, Loss: 10.2862, Val: 21.1090, Test: 21.3094\n",
      "Epoch: 39, Loss: 10.5110, Val: 10.1012, Test: 10.4698\n",
      "Epoch: 40, Loss: 10.2188, Val: 12.4706, Test: 12.3968\n",
      "Epoch: 41, Loss: 9.9576, Val: 15.7481, Test: 15.2038\n",
      "Epoch: 42, Loss: 9.9552, Val: 27.8358, Test: 28.4282\n",
      "Epoch: 43, Loss: 10.3652, Val: 21.3135, Test: 21.2182\n",
      "Epoch: 44, Loss: 10.2155, Val: 15.8331, Test: 15.3061\n",
      "Epoch: 45, Loss: 9.8126, Val: 12.0894, Test: 13.5808\n",
      "Epoch: 46, Loss: 9.7615, Val: 9.3176, Test: 10.0114\n",
      "Epoch: 47, Loss: 10.0417, Val: 12.4670, Test: 11.6668\n",
      "Epoch: 48, Loss: 9.8204, Val: 12.2726, Test: 13.3366\n",
      "Epoch: 49, Loss: 9.5873, Val: 9.4758, Test: 10.1631\n",
      "Epoch: 50, Loss: 9.2201, Val: 9.4234, Test: 10.2907\n",
      "Epoch: 51, Loss: 9.1012, Val: 11.8824, Test: 12.8660\n",
      "Epoch: 52, Loss: 9.2539, Val: 8.6386, Test: 9.7118\n",
      "Epoch: 53, Loss: 9.0442, Val: 9.3702, Test: 9.3241\n",
      "Epoch: 54, Loss: 9.1052, Val: 12.0416, Test: 12.2439\n",
      "Epoch: 55, Loss: 8.8952, Val: 9.6984, Test: 10.3056\n",
      "Epoch: 56, Loss: 9.0185, Val: 14.2920, Test: 13.4574\n",
      "Epoch: 57, Loss: 9.0857, Val: 14.3911, Test: 14.9218\n",
      "Epoch: 58, Loss: 8.9340, Val: 9.5579, Test: 9.3068\n",
      "Epoch: 59, Loss: 8.6636, Val: 9.2245, Test: 9.0592\n",
      "Epoch: 60, Loss: 8.8595, Val: 11.4469, Test: 11.4712\n",
      "Epoch: 61, Loss: 8.6521, Val: 10.8905, Test: 11.3080\n",
      "Epoch: 62, Loss: 8.7105, Val: 10.9238, Test: 11.6592\n",
      "Epoch: 63, Loss: 8.6625, Val: 17.0079, Test: 14.9630\n",
      "Epoch: 64, Loss: 8.5637, Val: 8.3965, Test: 9.1023\n",
      "Epoch: 65, Loss: 8.4482, Val: 10.1892, Test: 10.7225\n",
      "Epoch: 66, Loss: 8.5482, Val: 9.4006, Test: 10.0525\n",
      "Epoch: 67, Loss: 8.4762, Val: 8.1043, Test: 8.9988\n",
      "Epoch: 68, Loss: 8.2871, Val: 8.6650, Test: 9.5397\n",
      "Epoch: 69, Loss: 8.5670, Val: 10.3356, Test: 11.0817\n",
      "Epoch: 70, Loss: 8.4082, Val: 8.6700, Test: 9.2426\n",
      "Epoch: 71, Loss: 8.4112, Val: 10.8893, Test: 11.7177\n",
      "Epoch: 72, Loss: 8.4329, Val: 8.6061, Test: 8.8654\n",
      "Epoch: 73, Loss: 8.3571, Val: 9.0610, Test: 10.0608\n",
      "Epoch: 74, Loss: 8.2170, Val: 11.6986, Test: 12.4291\n",
      "Epoch: 75, Loss: 8.3478, Val: 8.3370, Test: 8.9498\n",
      "Epoch: 76, Loss: 8.2302, Val: 11.2340, Test: 10.5967\n",
      "Epoch: 77, Loss: 8.3404, Val: 8.1093, Test: 8.9698\n",
      "Epoch: 78, Loss: 8.3451, Val: 8.1207, Test: 8.9316\n",
      "Epoch: 79, Loss: 8.1311, Val: 8.2924, Test: 9.0295\n",
      "Epoch: 80, Loss: 8.1401, Val: 8.2504, Test: 9.0235\n",
      "Epoch: 81, Loss: 8.1676, Val: 8.1327, Test: 8.9176\n",
      "Epoch: 82, Loss: 8.1366, Val: 8.0323, Test: 8.9254\n",
      "Epoch: 83, Loss: 8.0425, Val: 8.5361, Test: 8.8528\n",
      "Epoch: 84, Loss: 8.0203, Val: 8.4770, Test: 9.2461\n",
      "Epoch: 85, Loss: 8.0010, Val: 8.6271, Test: 9.4120\n",
      "Epoch: 86, Loss: 7.9845, Val: 8.0946, Test: 8.9840\n",
      "Epoch: 87, Loss: 7.9233, Val: 8.0271, Test: 8.8841\n",
      "Epoch: 88, Loss: 7.9329, Val: 8.3621, Test: 9.2302\n",
      "Epoch: 89, Loss: 7.9781, Val: 9.2116, Test: 9.5294\n",
      "Epoch: 90, Loss: 7.8665, Val: 8.7037, Test: 9.3858\n",
      "Epoch: 91, Loss: 7.9118, Val: 8.9936, Test: 9.5025\n",
      "Epoch: 92, Loss: 7.8578, Val: 8.5661, Test: 9.4138\n",
      "Epoch: 93, Loss: 7.8802, Val: 8.4182, Test: 9.1698\n",
      "Epoch: 94, Loss: 7.9171, Val: 9.8498, Test: 10.6599\n",
      "Epoch: 95, Loss: 7.8130, Val: 8.8771, Test: 9.5542\n",
      "Epoch: 96, Loss: 7.9965, Val: 8.3723, Test: 9.1412\n",
      "Epoch: 97, Loss: 7.9732, Val: 8.4319, Test: 9.2735\n",
      "Epoch: 98, Loss: 7.8736, Val: 8.2881, Test: 9.0073\n",
      "Epoch: 99, Loss: 7.6961, Val: 8.0257, Test: 9.1097\n",
      "Epoch: 100, Loss: 7.8839, Val: 8.0697, Test: 8.9900\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "MAE=9.109708786010742\n",
      "Epoch=99\n"
     ]
    }
   ],
   "source": [
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=2, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='bn',\n",
    "                            gate=False,\n",
    "                            qkv_bias=False,\n",
    "                            gt_aggregators=['sum'],\n",
    "                            aggregators=['sum'],\n",
    "                            dropout=0.1,\n",
    "                            act='relu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "for epoch in range(1, 101):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader)\n",
    "    te_loss = test(test_loader)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67715f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
