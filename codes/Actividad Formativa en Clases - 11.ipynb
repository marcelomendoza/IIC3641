{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e0e42c",
   "metadata": {},
   "source": [
    "# IIC-3641 GML UC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d67e24-ad70-4e3d-ad62-98629137df29",
   "metadata": {},
   "source": [
    "## Actividad en clase\n",
    "\n",
    "Vamos a usar el VGAE para trabajar en esta actividad.\n",
    "\n",
    "- Corra el VGAE y vea que puede reproducir el ejemplo de la clase.\n",
    "- Use el dataset **pubmed** que dejé en la carpeta data de github. \n",
    "- Entrene el VGAE. Pruebe dos variantes del VGAE y evalúe no solo el accuracy en la tarea sino que la reconstrucción de A. Corra 100 epochs para cada variante.\n",
    "- Cuanto termine, me avisa para entregarle una **L (logrado)**.\n",
    "- Recuerde que las L otorgan un bono en la nota final de la asignatura.\n",
    "\n",
    "***Tiene hasta el final de la clase.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a67715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix, dia_matrix, lil_matrix, eye, vstack, isspmatrix_coo, coo_matrix, diags, triu\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def load_data(dataset):\n",
    "    # load the data: x, tx, allx, graph\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "\n",
    "    features = vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    return adj, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bac2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = coo_matrix(adj)\n",
    "    adj_ = adj + eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 10% positive links\n",
    "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "    # TODO: Clean up.\n",
    "\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c0da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'pubmed'\n",
    "model = 'VGAE'\n",
    "\n",
    "input_dim = 500 \n",
    "hidden1_dim = 32\n",
    "hidden2_dim = 16\n",
    "use_feature = True\n",
    "\n",
    "num_epoch = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc14689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class VGAE(nn.Module):\n",
    "    def __init__(self, adj):\n",
    "        super(VGAE,self).__init__()\n",
    "        self.base_gcn = GraphConvSparse(input_dim, hidden1_dim, adj)\n",
    "        self.gcn_mean = GraphConvSparse(hidden1_dim, hidden2_dim, adj, activation=lambda x:x)\n",
    "        self.gcn_logstddev = GraphConvSparse(hidden1_dim, hidden2_dim, adj, activation=lambda x:x)\n",
    "\n",
    "    def encode(self, X):\n",
    "        hidden = self.base_gcn(X)\n",
    "        self.mean = self.gcn_mean(hidden)\n",
    "        self.logstd = self.gcn_logstddev(hidden)\n",
    "        gaussian_noise = torch.randn(X.size(0), hidden2_dim)\n",
    "        sampled_z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "        return sampled_z\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.encode(X)\n",
    "        A_pred = dot_product_decode(Z)\n",
    "        return A_pred\n",
    "\n",
    "    \n",
    "class GraphConvSparse(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, adj, activation = F.relu, **kwargs):\n",
    "        super(GraphConvSparse, self).__init__(**kwargs)\n",
    "        self.weight = glorot_init(input_dim, output_dim) \n",
    "        self.adj = adj\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = torch.mm(x,self.weight)\n",
    "        x = torch.mm(self.adj, x)\n",
    "        outputs = self.activation(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def dot_product_decode(Z):\n",
    "    A_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "    return A_pred\n",
    "\n",
    "def glorot_init(input_dim, output_dim):\n",
    "    init_range = np.sqrt(6.0/(input_dim + output_dim))\n",
    "    initial = torch.rand(input_dim, output_dim)*2*init_range - init_range\n",
    "    return nn.Parameter(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4347bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146095/276524816.py:20: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import time\n",
    "\n",
    "\n",
    "# Train on CPU (hide GPU) due to memory constraints\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "adj, features = load_data(dataset)\n",
    "\n",
    "A = adj\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c0ed8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19717, 19717)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "370b7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146095/503417153.py:8: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:641.)\n",
      "  adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T),\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "adj_label = adj_train + eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                            torch.FloatTensor(features[1]), \n",
    "                            torch.Size(features[2]))\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2aba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGAE(adj_norm)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        # print(e)\n",
    "        # print(adj_rec[e[0], e[1]])\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score\n",
    "\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "165ce65f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.74199 train_acc= 0.50001 val_roc= 0.49352 val_ap= 0.50256 time= 3.72806\n",
      "Epoch: 0002 train_loss= 1.70254 train_acc= 0.49977 val_roc= 0.49066 val_ap= 0.48943 time= 3.71223\n",
      "Epoch: 0003 train_loss= 1.67070 train_acc= 0.49861 val_roc= 0.48446 val_ap= 0.48707 time= 3.70257\n",
      "Epoch: 0004 train_loss= 1.60658 train_acc= 0.49624 val_roc= 0.51563 val_ap= 0.50776 time= 3.71039\n",
      "Epoch: 0005 train_loss= 1.54948 train_acc= 0.49252 val_roc= 0.50333 val_ap= 0.49554 time= 3.66675\n",
      "Epoch: 0006 train_loss= 1.49437 train_acc= 0.48589 val_roc= 0.52313 val_ap= 0.51392 time= 3.66979\n",
      "Epoch: 0007 train_loss= 1.43495 train_acc= 0.47502 val_roc= 0.51132 val_ap= 0.51271 time= 3.67103\n",
      "Epoch: 0008 train_loss= 1.35413 train_acc= 0.45943 val_roc= 0.52758 val_ap= 0.52328 time= 3.87068\n",
      "Epoch: 0009 train_loss= 1.29856 train_acc= 0.43834 val_roc= 0.53999 val_ap= 0.52796 time= 3.75251\n",
      "Epoch: 0010 train_loss= 1.23275 train_acc= 0.41456 val_roc= 0.56145 val_ap= 0.55194 time= 3.77590\n",
      "Epoch: 0011 train_loss= 1.17725 train_acc= 0.39194 val_roc= 0.56885 val_ap= 0.56402 time= 3.67399\n",
      "Epoch: 0012 train_loss= 1.11602 train_acc= 0.37935 val_roc= 0.58047 val_ap= 0.57235 time= 3.72788\n",
      "Epoch: 0013 train_loss= 1.06336 train_acc= 0.36613 val_roc= 0.58834 val_ap= 0.57589 time= 3.71501\n",
      "Epoch: 0014 train_loss= 1.00017 train_acc= 0.36251 val_roc= 0.60605 val_ap= 0.57978 time= 3.70787\n",
      "Epoch: 0015 train_loss= 0.95268 train_acc= 0.35971 val_roc= 0.60427 val_ap= 0.57476 time= 3.71729\n",
      "Epoch: 0016 train_loss= 0.90246 train_acc= 0.36205 val_roc= 0.61462 val_ap= 0.58609 time= 3.77536\n",
      "Epoch: 0017 train_loss= 0.86028 train_acc= 0.36472 val_roc= 0.59858 val_ap= 0.57433 time= 3.67078\n",
      "Epoch: 0018 train_loss= 0.82381 train_acc= 0.37052 val_roc= 0.59635 val_ap= 0.56118 time= 3.69475\n",
      "Epoch: 0019 train_loss= 0.78883 train_acc= 0.37636 val_roc= 0.60479 val_ap= 0.56165 time= 3.69837\n",
      "Epoch: 0020 train_loss= 0.76713 train_acc= 0.38213 val_roc= 0.58500 val_ap= 0.54157 time= 3.68100\n",
      "Epoch: 0021 train_loss= 0.74881 train_acc= 0.38345 val_roc= 0.60911 val_ap= 0.55826 time= 3.70856\n",
      "Epoch: 0022 train_loss= 0.73514 train_acc= 0.37746 val_roc= 0.59944 val_ap= 0.54610 time= 3.71908\n",
      "Epoch: 0023 train_loss= 0.72387 train_acc= 0.36452 val_roc= 0.60588 val_ap= 0.54409 time= 3.71480\n",
      "Epoch: 0024 train_loss= 0.71336 train_acc= 0.34244 val_roc= 0.64082 val_ap= 0.58431 time= 3.69941\n",
      "Epoch: 0025 train_loss= 0.70427 train_acc= 0.30956 val_roc= 0.65400 val_ap= 0.61361 time= 3.71155\n",
      "Epoch: 0026 train_loss= 0.70026 train_acc= 0.26786 val_roc= 0.67688 val_ap= 0.65160 time= 3.73357\n",
      "Epoch: 0027 train_loss= 0.69792 train_acc= 0.23859 val_roc= 0.69670 val_ap= 0.68264 time= 3.74783\n",
      "Epoch: 0028 train_loss= 0.69455 train_acc= 0.22132 val_roc= 0.71510 val_ap= 0.70902 time= 3.69420\n",
      "Epoch: 0029 train_loss= 0.69096 train_acc= 0.21531 val_roc= 0.71082 val_ap= 0.70975 time= 3.68741\n",
      "Epoch: 0030 train_loss= 0.68983 train_acc= 0.21601 val_roc= 0.70425 val_ap= 0.69921 time= 3.71012\n",
      "Epoch: 0031 train_loss= 0.68488 train_acc= 0.22621 val_roc= 0.70784 val_ap= 0.70247 time= 3.70655\n",
      "Epoch: 0032 train_loss= 0.68568 train_acc= 0.22672 val_roc= 0.70037 val_ap= 0.69316 time= 3.70669\n",
      "Epoch: 0033 train_loss= 0.68536 train_acc= 0.21791 val_roc= 0.71826 val_ap= 0.71495 time= 3.75330\n",
      "Epoch: 0034 train_loss= 0.68370 train_acc= 0.20140 val_roc= 0.72656 val_ap= 0.72429 time= 3.74330\n",
      "Epoch: 0035 train_loss= 0.68311 train_acc= 0.17855 val_roc= 0.74548 val_ap= 0.75361 time= 3.84197\n",
      "Epoch: 0036 train_loss= 0.68275 train_acc= 0.16127 val_roc= 0.76291 val_ap= 0.77142 time= 3.80657\n",
      "Epoch: 0037 train_loss= 0.68322 train_acc= 0.14937 val_roc= 0.76019 val_ap= 0.77254 time= 3.76443\n",
      "Epoch: 0038 train_loss= 0.68219 train_acc= 0.14739 val_roc= 0.76069 val_ap= 0.77160 time= 3.81008\n",
      "Epoch: 0039 train_loss= 0.68037 train_acc= 0.15712 val_roc= 0.75569 val_ap= 0.76740 time= 3.76230\n",
      "Epoch: 0040 train_loss= 0.67960 train_acc= 0.16638 val_roc= 0.75063 val_ap= 0.76751 time= 3.78302\n",
      "Epoch: 0041 train_loss= 0.67948 train_acc= 0.17567 val_roc= 0.74816 val_ap= 0.76501 time= 3.76664\n",
      "Epoch: 0042 train_loss= 0.67789 train_acc= 0.17700 val_roc= 0.75362 val_ap= 0.76913 time= 3.86289\n",
      "Epoch: 0043 train_loss= 0.67703 train_acc= 0.17099 val_roc= 0.75152 val_ap= 0.77229 time= 3.71065\n",
      "Epoch: 0044 train_loss= 0.67632 train_acc= 0.16335 val_roc= 0.75900 val_ap= 0.77618 time= 3.78459\n",
      "Epoch: 0045 train_loss= 0.67543 train_acc= 0.15594 val_roc= 0.76119 val_ap= 0.77960 time= 3.68607\n",
      "Epoch: 0046 train_loss= 0.67507 train_acc= 0.15720 val_roc= 0.75753 val_ap= 0.77573 time= 3.70681\n",
      "Epoch: 0047 train_loss= 0.67345 train_acc= 0.16655 val_roc= 0.75811 val_ap= 0.78180 time= 3.80554\n",
      "Epoch: 0048 train_loss= 0.67279 train_acc= 0.17483 val_roc= 0.75156 val_ap= 0.77275 time= 3.80157\n",
      "Epoch: 0049 train_loss= 0.67042 train_acc= 0.19262 val_roc= 0.75531 val_ap= 0.77554 time= 3.71517\n",
      "Epoch: 0050 train_loss= 0.66928 train_acc= 0.20732 val_roc= 0.73483 val_ap= 0.76090 time= 3.79133\n",
      "Epoch: 0051 train_loss= 0.66708 train_acc= 0.21573 val_roc= 0.74560 val_ap= 0.77121 time= 3.70995\n",
      "Epoch: 0052 train_loss= 0.66515 train_acc= 0.21622 val_roc= 0.74839 val_ap= 0.77245 time= 3.82752\n",
      "Epoch: 0053 train_loss= 0.66273 train_acc= 0.21242 val_roc= 0.75502 val_ap= 0.77757 time= 3.71010\n",
      "Epoch: 0054 train_loss= 0.66047 train_acc= 0.21146 val_roc= 0.75933 val_ap= 0.78140 time= 3.68461\n",
      "Epoch: 0055 train_loss= 0.65655 train_acc= 0.21183 val_roc= 0.77034 val_ap= 0.79257 time= 3.68038\n",
      "Epoch: 0056 train_loss= 0.65331 train_acc= 0.21485 val_roc= 0.78561 val_ap= 0.80233 time= 3.71471\n",
      "Epoch: 0057 train_loss= 0.64893 train_acc= 0.22520 val_roc= 0.78884 val_ap= 0.80526 time= 3.68533\n",
      "Epoch: 0058 train_loss= 0.64442 train_acc= 0.23501 val_roc= 0.80430 val_ap= 0.81766 time= 3.67892\n",
      "Epoch: 0059 train_loss= 0.64025 train_acc= 0.25071 val_roc= 0.80549 val_ap= 0.81823 time= 3.70889\n",
      "Epoch: 0060 train_loss= 0.63515 train_acc= 0.27282 val_roc= 0.82250 val_ap= 0.83050 time= 3.83501\n",
      "Epoch: 0061 train_loss= 0.62934 train_acc= 0.29588 val_roc= 0.81795 val_ap= 0.82794 time= 3.68059\n",
      "Epoch: 0062 train_loss= 0.62389 train_acc= 0.32028 val_roc= 0.81999 val_ap= 0.82905 time= 3.67593\n",
      "Epoch: 0063 train_loss= 0.61649 train_acc= 0.34545 val_roc= 0.82203 val_ap= 0.83058 time= 3.75581\n",
      "Epoch: 0064 train_loss= 0.61005 train_acc= 0.36846 val_roc= 0.82610 val_ap= 0.83553 time= 3.72324\n",
      "Epoch: 0065 train_loss= 0.60416 train_acc= 0.38818 val_roc= 0.82436 val_ap= 0.83267 time= 3.69984\n",
      "Epoch: 0066 train_loss= 0.59800 train_acc= 0.40759 val_roc= 0.82906 val_ap= 0.83771 time= 3.71565\n",
      "Epoch: 0067 train_loss= 0.59249 train_acc= 0.42319 val_roc= 0.82738 val_ap= 0.83529 time= 3.72109\n",
      "Epoch: 0068 train_loss= 0.58869 train_acc= 0.43314 val_roc= 0.82747 val_ap= 0.83428 time= 3.74123\n",
      "Epoch: 0069 train_loss= 0.58562 train_acc= 0.43882 val_roc= 0.82649 val_ap= 0.83578 time= 4.05236\n",
      "Epoch: 0070 train_loss= 0.58384 train_acc= 0.44519 val_roc= 0.82230 val_ap= 0.83276 time= 3.73990\n",
      "Epoch: 0071 train_loss= 0.58349 train_acc= 0.44920 val_roc= 0.82860 val_ap= 0.83892 time= 3.81975\n",
      "Epoch: 0072 train_loss= 0.58450 train_acc= 0.45392 val_roc= 0.82878 val_ap= 0.83870 time= 3.79028\n",
      "Epoch: 0073 train_loss= 0.58554 train_acc= 0.45876 val_roc= 0.82599 val_ap= 0.83973 time= 3.75875\n",
      "Epoch: 0074 train_loss= 0.58647 train_acc= 0.46408 val_roc= 0.82561 val_ap= 0.83738 time= 3.82313\n",
      "Epoch: 0075 train_loss= 0.58795 train_acc= 0.46604 val_roc= 0.82566 val_ap= 0.83750 time= 3.78659\n",
      "Epoch: 0076 train_loss= 0.58882 train_acc= 0.46579 val_roc= 0.82726 val_ap= 0.83862 time= 3.78563\n",
      "Epoch: 0077 train_loss= 0.58788 train_acc= 0.46350 val_roc= 0.82669 val_ap= 0.83929 time= 3.73832\n",
      "Epoch: 0078 train_loss= 0.58644 train_acc= 0.46066 val_roc= 0.82760 val_ap= 0.84154 time= 3.82467\n",
      "Epoch: 0079 train_loss= 0.58526 train_acc= 0.45958 val_roc= 0.82792 val_ap= 0.84079 time= 3.75107\n",
      "Epoch: 0080 train_loss= 0.58322 train_acc= 0.45940 val_roc= 0.82849 val_ap= 0.84088 time= 3.70008\n",
      "Epoch: 0081 train_loss= 0.58059 train_acc= 0.46104 val_roc= 0.82942 val_ap= 0.83942 time= 3.71312\n",
      "Epoch: 0082 train_loss= 0.57862 train_acc= 0.46256 val_roc= 0.82562 val_ap= 0.83878 time= 3.69253\n",
      "Epoch: 0083 train_loss= 0.57733 train_acc= 0.46376 val_roc= 0.82590 val_ap= 0.83893 time= 3.74620\n",
      "Epoch: 0084 train_loss= 0.57653 train_acc= 0.46298 val_roc= 0.83084 val_ap= 0.84404 time= 3.72582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0085 train_loss= 0.57499 train_acc= 0.46392 val_roc= 0.82834 val_ap= 0.83978 time= 3.72192\n",
      "Epoch: 0086 train_loss= 0.57502 train_acc= 0.46356 val_roc= 0.82765 val_ap= 0.83920 time= 3.69672\n",
      "Epoch: 0087 train_loss= 0.57498 train_acc= 0.46224 val_roc= 0.82528 val_ap= 0.83829 time= 3.76685\n",
      "Epoch: 0088 train_loss= 0.57450 train_acc= 0.46255 val_roc= 0.82439 val_ap= 0.83779 time= 3.74239\n",
      "Epoch: 0089 train_loss= 0.57459 train_acc= 0.46361 val_roc= 0.82649 val_ap= 0.83660 time= 3.73788\n",
      "Epoch: 0090 train_loss= 0.57462 train_acc= 0.46516 val_roc= 0.82574 val_ap= 0.83672 time= 3.77322\n",
      "Epoch: 0091 train_loss= 0.57382 train_acc= 0.46706 val_roc= 0.82704 val_ap= 0.83944 time= 3.86350\n",
      "Epoch: 0092 train_loss= 0.57322 train_acc= 0.46887 val_roc= 0.83197 val_ap= 0.84509 time= 3.82832\n",
      "Epoch: 0093 train_loss= 0.57330 train_acc= 0.46805 val_roc= 0.82779 val_ap= 0.83858 time= 3.78912\n",
      "Epoch: 0094 train_loss= 0.57220 train_acc= 0.46734 val_roc= 0.83508 val_ap= 0.84537 time= 3.77758\n",
      "Epoch: 0095 train_loss= 0.57118 train_acc= 0.46724 val_roc= 0.83382 val_ap= 0.84450 time= 3.89370\n",
      "Epoch: 0096 train_loss= 0.57071 train_acc= 0.46646 val_roc= 0.83404 val_ap= 0.84511 time= 3.80657\n",
      "Epoch: 0097 train_loss= 0.56965 train_acc= 0.46682 val_roc= 0.83533 val_ap= 0.84454 time= 3.73845\n",
      "Epoch: 0098 train_loss= 0.56927 train_acc= 0.46799 val_roc= 0.82829 val_ap= 0.84069 time= 3.71708\n",
      "Epoch: 0099 train_loss= 0.56834 train_acc= 0.47096 val_roc= 0.83557 val_ap= 0.84839 time= 3.72609\n",
      "Epoch: 0100 train_loss= 0.56830 train_acc= 0.47178 val_roc= 0.83628 val_ap= 0.84842 time= 3.70716\n",
      "End of training! test_roc= 0.83270 test_ap= 0.84511\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    t = time.time()\n",
    "\n",
    "    A_pred = model(features)\n",
    "    optimizer.zero_grad()\n",
    "    loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight = weight_tensor)\n",
    "    kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(A_pred,adj_label)\n",
    "\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, A_pred)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_roc=\", \"{:.5f}\".format(val_roc),\n",
    "          \"val_ap=\", \"{:.5f}\".format(val_ap),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, A_pred)\n",
    "print(\"End of training!\", \"test_roc=\", \"{:.5f}\".format(test_roc),\n",
    "      \"test_ap=\", \"{:.5f}\".format(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bada325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6777e+08, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Z = model.encode(features)\n",
    "A_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "dense_A = torch.tensor(A.toarray())\n",
    "l1_diff = torch.norm(A_pred - dense_A, p=1)\n",
    "print(l1_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e6cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146095/276524816.py:20: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  objects.append(pkl.load(f, encoding='latin1'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.32252 train_acc= 0.49992 val_roc= 0.51164 val_ap= 0.51012 time= 3.66858\n",
      "Epoch: 0002 train_loss= 1.28053 train_acc= 0.49968 val_roc= 0.50459 val_ap= 0.49802 time= 3.74474\n",
      "Epoch: 0003 train_loss= 1.25676 train_acc= 0.49905 val_roc= 0.50801 val_ap= 0.50366 time= 3.73450\n",
      "Epoch: 0004 train_loss= 1.24472 train_acc= 0.49699 val_roc= 0.50140 val_ap= 0.50404 time= 3.71059\n",
      "Epoch: 0005 train_loss= 1.19477 train_acc= 0.49449 val_roc= 0.51429 val_ap= 0.50667 time= 3.87550\n",
      "Epoch: 0006 train_loss= 1.17450 train_acc= 0.49005 val_roc= 0.51093 val_ap= 0.49438 time= 3.78142\n",
      "Epoch: 0007 train_loss= 1.13654 train_acc= 0.48233 val_roc= 0.51329 val_ap= 0.50293 time= 3.76425\n",
      "Epoch: 0008 train_loss= 1.08973 train_acc= 0.47345 val_roc= 0.50779 val_ap= 0.50264 time= 3.76641\n",
      "Epoch: 0009 train_loss= 1.06274 train_acc= 0.45859 val_roc= 0.53389 val_ap= 0.52653 time= 3.73679\n",
      "Epoch: 0010 train_loss= 1.02962 train_acc= 0.44248 val_roc= 0.54375 val_ap= 0.53495 time= 3.77916\n",
      "Epoch: 0011 train_loss= 0.99206 train_acc= 0.42671 val_roc= 0.55340 val_ap= 0.54270 time= 3.75875\n",
      "Epoch: 0012 train_loss= 0.95687 train_acc= 0.41373 val_roc= 0.56632 val_ap= 0.55452 time= 3.73770\n",
      "Epoch: 0013 train_loss= 0.92805 train_acc= 0.40511 val_roc= 0.56625 val_ap= 0.55823 time= 3.82025\n",
      "Epoch: 0014 train_loss= 0.90233 train_acc= 0.39382 val_roc= 0.58975 val_ap= 0.56960 time= 3.75180\n",
      "Epoch: 0015 train_loss= 0.87470 train_acc= 0.38914 val_roc= 0.57522 val_ap= 0.55447 time= 3.73228\n",
      "Epoch: 0016 train_loss= 0.84424 train_acc= 0.38854 val_roc= 0.59905 val_ap= 0.57393 time= 3.73237\n",
      "Epoch: 0017 train_loss= 0.82606 train_acc= 0.38294 val_roc= 0.59138 val_ap= 0.57550 time= 3.75726\n",
      "Epoch: 0018 train_loss= 0.80169 train_acc= 0.38705 val_roc= 0.60104 val_ap= 0.57779 time= 3.77073\n",
      "Epoch: 0019 train_loss= 0.78442 train_acc= 0.38764 val_roc= 0.58478 val_ap= 0.56274 time= 3.81463\n",
      "Epoch: 0020 train_loss= 0.76747 train_acc= 0.39272 val_roc= 0.60358 val_ap= 0.56629 time= 3.80734\n",
      "Epoch: 0021 train_loss= 0.75380 train_acc= 0.39479 val_roc= 0.59996 val_ap= 0.56103 time= 3.73702\n",
      "Epoch: 0022 train_loss= 0.74550 train_acc= 0.39320 val_roc= 0.58626 val_ap= 0.54378 time= 3.74396\n",
      "Epoch: 0023 train_loss= 0.73552 train_acc= 0.39058 val_roc= 0.61002 val_ap= 0.56504 time= 3.74134\n",
      "Epoch: 0024 train_loss= 0.72629 train_acc= 0.38468 val_roc= 0.58397 val_ap= 0.54455 time= 3.88529\n",
      "Epoch: 0025 train_loss= 0.72012 train_acc= 0.37352 val_roc= 0.60696 val_ap= 0.57274 time= 3.73879\n",
      "Epoch: 0026 train_loss= 0.71377 train_acc= 0.35368 val_roc= 0.63252 val_ap= 0.60011 time= 3.87047\n",
      "Epoch: 0027 train_loss= 0.70755 train_acc= 0.33095 val_roc= 0.64862 val_ap= 0.61800 time= 3.78081\n",
      "Epoch: 0028 train_loss= 0.70386 train_acc= 0.30555 val_roc= 0.65492 val_ap= 0.63574 time= 3.75831\n",
      "Epoch: 0029 train_loss= 0.69945 train_acc= 0.27946 val_roc= 0.68945 val_ap= 0.68089 time= 3.74942\n",
      "Epoch: 0030 train_loss= 0.69837 train_acc= 0.25108 val_roc= 0.70427 val_ap= 0.70359 time= 3.75200\n",
      "Epoch: 0031 train_loss= 0.69568 train_acc= 0.23508 val_roc= 0.72191 val_ap= 0.72224 time= 3.76412\n",
      "Epoch: 0032 train_loss= 0.69479 train_acc= 0.22455 val_roc= 0.71311 val_ap= 0.72241 time= 3.75437\n",
      "Epoch: 0033 train_loss= 0.69125 train_acc= 0.22306 val_roc= 0.72713 val_ap= 0.73164 time= 3.80285\n",
      "Epoch: 0034 train_loss= 0.68916 train_acc= 0.22516 val_roc= 0.72403 val_ap= 0.73415 time= 3.82710\n",
      "Epoch: 0035 train_loss= 0.68747 train_acc= 0.22869 val_roc= 0.72507 val_ap= 0.73811 time= 3.84558\n",
      "Epoch: 0036 train_loss= 0.68616 train_acc= 0.23473 val_roc= 0.72520 val_ap= 0.73505 time= 3.85578\n",
      "Epoch: 0037 train_loss= 0.68492 train_acc= 0.23766 val_roc= 0.71541 val_ap= 0.72958 time= 3.75855\n",
      "Epoch: 0038 train_loss= 0.68369 train_acc= 0.23528 val_roc= 0.72591 val_ap= 0.74176 time= 3.77182\n",
      "Epoch: 0039 train_loss= 0.68298 train_acc= 0.23146 val_roc= 0.72787 val_ap= 0.74399 time= 3.81061\n",
      "Epoch: 0040 train_loss= 0.68145 train_acc= 0.22672 val_roc= 0.73223 val_ap= 0.75279 time= 3.77605\n",
      "Epoch: 0041 train_loss= 0.68083 train_acc= 0.21408 val_roc= 0.73710 val_ap= 0.75812 time= 3.75193\n",
      "Epoch: 0042 train_loss= 0.67979 train_acc= 0.20863 val_roc= 0.73417 val_ap= 0.75369 time= 3.82840\n",
      "Epoch: 0043 train_loss= 0.67879 train_acc= 0.20618 val_roc= 0.74249 val_ap= 0.76436 time= 3.76391\n",
      "Epoch: 0044 train_loss= 0.67783 train_acc= 0.20738 val_roc= 0.74767 val_ap= 0.77003 time= 3.78457\n",
      "Epoch: 0045 train_loss= 0.67649 train_acc= 0.21218 val_roc= 0.73694 val_ap= 0.75943 time= 3.75403\n",
      "Epoch: 0046 train_loss= 0.67428 train_acc= 0.22480 val_roc= 0.73037 val_ap= 0.75602 time= 3.93154\n",
      "Epoch: 0047 train_loss= 0.67149 train_acc= 0.23621 val_roc= 0.74307 val_ap= 0.76702 time= 3.73324\n",
      "Epoch: 0048 train_loss= 0.66986 train_acc= 0.24325 val_roc= 0.73786 val_ap= 0.75938 time= 3.77284\n",
      "Epoch: 0049 train_loss= 0.66770 train_acc= 0.24327 val_roc= 0.75643 val_ap= 0.77734 time= 3.75669\n",
      "Epoch: 0050 train_loss= 0.66473 train_acc= 0.24646 val_roc= 0.75056 val_ap= 0.77022 time= 3.76090\n",
      "Epoch: 0051 train_loss= 0.66143 train_acc= 0.24559 val_roc= 0.76139 val_ap= 0.78105 time= 3.85680\n",
      "Epoch: 0052 train_loss= 0.65712 train_acc= 0.24139 val_roc= 0.78960 val_ap= 0.80180 time= 3.76021\n",
      "Epoch: 0053 train_loss= 0.65461 train_acc= 0.23937 val_roc= 0.79105 val_ap= 0.80577 time= 3.76904\n",
      "Epoch: 0054 train_loss= 0.65148 train_acc= 0.24053 val_roc= 0.79569 val_ap= 0.80728 time= 3.78375\n",
      "Epoch: 0055 train_loss= 0.64678 train_acc= 0.25049 val_roc= 0.80906 val_ap= 0.81815 time= 3.78162\n",
      "Epoch: 0056 train_loss= 0.64159 train_acc= 0.26832 val_roc= 0.81192 val_ap= 0.82265 time= 3.73466\n",
      "Epoch: 0057 train_loss= 0.63668 train_acc= 0.29020 val_roc= 0.81071 val_ap= 0.81899 time= 3.85541\n",
      "Epoch: 0058 train_loss= 0.63090 train_acc= 0.31633 val_roc= 0.80775 val_ap= 0.81500 time= 3.78335\n",
      "Epoch: 0059 train_loss= 0.62523 train_acc= 0.34294 val_roc= 0.81140 val_ap= 0.82152 time= 3.83179\n",
      "Epoch: 0060 train_loss= 0.61956 train_acc= 0.36898 val_roc= 0.81541 val_ap= 0.82333 time= 3.77901\n",
      "Epoch: 0061 train_loss= 0.61442 train_acc= 0.39276 val_roc= 0.80816 val_ap= 0.81458 time= 3.74930\n",
      "Epoch: 0062 train_loss= 0.60970 train_acc= 0.40965 val_roc= 0.80436 val_ap= 0.81179 time= 3.75537\n",
      "Epoch: 0063 train_loss= 0.60483 train_acc= 0.42424 val_roc= 0.81070 val_ap= 0.81885 time= 3.79409\n",
      "Epoch: 0064 train_loss= 0.60091 train_acc= 0.43219 val_roc= 0.81099 val_ap= 0.81691 time= 3.82146\n",
      "Epoch: 0065 train_loss= 0.59749 train_acc= 0.43869 val_roc= 0.81451 val_ap= 0.82357 time= 3.79519\n",
      "Epoch: 0066 train_loss= 0.59530 train_acc= 0.44318 val_roc= 0.81712 val_ap= 0.82504 time= 3.83504\n",
      "Epoch: 0067 train_loss= 0.59424 train_acc= 0.44715 val_roc= 0.81242 val_ap= 0.82288 time= 3.83907\n",
      "Epoch: 0068 train_loss= 0.59365 train_acc= 0.45161 val_roc= 0.82154 val_ap= 0.82947 time= 3.85060\n",
      "Epoch: 0069 train_loss= 0.59367 train_acc= 0.45558 val_roc= 0.81892 val_ap= 0.82926 time= 3.81224\n",
      "Epoch: 0070 train_loss= 0.59384 train_acc= 0.45905 val_roc= 0.81874 val_ap= 0.82875 time= 3.73471\n",
      "Epoch: 0071 train_loss= 0.59438 train_acc= 0.46198 val_roc= 0.81576 val_ap= 0.82861 time= 3.73050\n",
      "Epoch: 0072 train_loss= 0.59481 train_acc= 0.46344 val_roc= 0.81868 val_ap= 0.83009 time= 3.82723\n",
      "Epoch: 0073 train_loss= 0.59519 train_acc= 0.46223 val_roc= 0.81719 val_ap= 0.82793 time= 3.76373\n",
      "Epoch: 0074 train_loss= 0.59511 train_acc= 0.46057 val_roc= 0.81870 val_ap= 0.82941 time= 3.74744\n",
      "Epoch: 0075 train_loss= 0.59380 train_acc= 0.45934 val_roc= 0.82774 val_ap= 0.83796 time= 3.75246\n",
      "Epoch: 0076 train_loss= 0.59179 train_acc= 0.45987 val_roc= 0.82329 val_ap= 0.83194 time= 3.76723\n",
      "Epoch: 0077 train_loss= 0.59106 train_acc= 0.45911 val_roc= 0.82321 val_ap= 0.83239 time= 3.76102\n",
      "Epoch: 0078 train_loss= 0.58992 train_acc= 0.45912 val_roc= 0.82467 val_ap= 0.83651 time= 3.75247\n",
      "Epoch: 0079 train_loss= 0.58783 train_acc= 0.46082 val_roc= 0.82546 val_ap= 0.83551 time= 3.75765\n",
      "Epoch: 0080 train_loss= 0.58632 train_acc= 0.46265 val_roc= 0.82585 val_ap= 0.83729 time= 3.73266\n",
      "Epoch: 0081 train_loss= 0.58539 train_acc= 0.46255 val_roc= 0.82459 val_ap= 0.83606 time= 3.73352\n",
      "Epoch: 0082 train_loss= 0.58409 train_acc= 0.46503 val_roc= 0.82888 val_ap= 0.83868 time= 3.78281\n",
      "Epoch: 0083 train_loss= 0.58286 train_acc= 0.46498 val_roc= 0.82206 val_ap= 0.83226 time= 3.76288\n",
      "Epoch: 0084 train_loss= 0.58200 train_acc= 0.46418 val_roc= 0.82476 val_ap= 0.83609 time= 3.85640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0085 train_loss= 0.58116 train_acc= 0.46430 val_roc= 0.82751 val_ap= 0.83878 time= 3.97235\n",
      "Epoch: 0086 train_loss= 0.58063 train_acc= 0.46421 val_roc= 0.82567 val_ap= 0.83708 time= 3.79700\n",
      "Epoch: 0087 train_loss= 0.57949 train_acc= 0.46398 val_roc= 0.82700 val_ap= 0.83667 time= 3.74549\n",
      "Epoch: 0088 train_loss= 0.57943 train_acc= 0.46403 val_roc= 0.83008 val_ap= 0.83987 time= 3.78613\n",
      "Epoch: 0089 train_loss= 0.57819 train_acc= 0.46486 val_roc= 0.82751 val_ap= 0.83668 time= 3.76624\n",
      "Epoch: 0090 train_loss= 0.57670 train_acc= 0.46781 val_roc= 0.83428 val_ap= 0.84328 time= 3.76623\n",
      "Epoch: 0091 train_loss= 0.57607 train_acc= 0.46725 val_roc= 0.83493 val_ap= 0.84416 time= 3.75100\n",
      "Epoch: 0092 train_loss= 0.57465 train_acc= 0.46739 val_roc= 0.83654 val_ap= 0.84405 time= 3.75778\n",
      "Epoch: 0093 train_loss= 0.57320 train_acc= 0.46548 val_roc= 0.84162 val_ap= 0.84982 time= 3.79701\n",
      "Epoch: 0094 train_loss= 0.57188 train_acc= 0.46625 val_roc= 0.83731 val_ap= 0.84580 time= 3.83603\n",
      "Epoch: 0095 train_loss= 0.57009 train_acc= 0.46640 val_roc= 0.83846 val_ap= 0.84780 time= 3.83110\n",
      "Epoch: 0096 train_loss= 0.56809 train_acc= 0.46857 val_roc= 0.84674 val_ap= 0.85257 time= 3.82752\n",
      "Epoch: 0097 train_loss= 0.56716 train_acc= 0.46859 val_roc= 0.84873 val_ap= 0.85707 time= 3.78811\n",
      "Epoch: 0098 train_loss= 0.56512 train_acc= 0.47140 val_roc= 0.84996 val_ap= 0.85758 time= 3.75126\n",
      "Epoch: 0099 train_loss= 0.56279 train_acc= 0.47350 val_roc= 0.85373 val_ap= 0.86141 time= 3.75248\n",
      "Epoch: 0100 train_loss= 0.56112 train_acc= 0.47360 val_roc= 0.85743 val_ap= 0.86475 time= 3.89739\n",
      "End of training! test_roc= 0.85353 test_ap= 0.86108\n"
     ]
    }
   ],
   "source": [
    "hidden1_dim = 16\n",
    "hidden2_dim = 8\n",
    "\n",
    "adj, features = load_data(dataset)\n",
    "\n",
    "A = adj\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "\n",
    "\n",
    "num_nodes = adj.shape[0]\n",
    "\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "# Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "adj_label = adj_train + eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(torch.LongTensor(adj_norm[0].T), \n",
    "                            torch.FloatTensor(adj_norm[1]), \n",
    "                            torch.Size(adj_norm[2]))\n",
    "adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T), \n",
    "                            torch.FloatTensor(adj_label[1]), \n",
    "                            torch.Size(adj_label[2]))\n",
    "features = torch.sparse.FloatTensor(torch.LongTensor(features[0].T), \n",
    "                            torch.FloatTensor(features[1]), \n",
    "                            torch.Size(features[2]))\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0)) \n",
    "weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "model = VGAE(adj_norm)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    t = time.time()\n",
    "\n",
    "    A_pred = model(features)\n",
    "    optimizer.zero_grad()\n",
    "    loss = log_lik = norm*F.binary_cross_entropy(A_pred.view(-1), adj_label.to_dense().view(-1), weight = weight_tensor)\n",
    "    kl_divergence = 0.5/ A_pred.size(0) * (1 + 2*model.logstd - model.mean**2 - torch.exp(model.logstd)**2).sum(1).mean()\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(A_pred,adj_label)\n",
    "\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, A_pred)\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss.item()),\n",
    "          \"train_acc=\", \"{:.5f}\".format(train_acc), \"val_roc=\", \"{:.5f}\".format(val_roc),\n",
    "          \"val_ap=\", \"{:.5f}\".format(val_ap),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, A_pred)\n",
    "print(\"End of training!\", \"test_roc=\", \"{:.5f}\".format(test_roc),\n",
    "      \"test_ap=\", \"{:.5f}\".format(test_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e839501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6777e+08, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Z = model.encode(features)\n",
    "A_pred = torch.sigmoid(torch.matmul(Z,Z.t()))\n",
    "dense_A = torch.tensor(A.toarray())\n",
    "l1_diff = torch.norm(A_pred - dense_A, p=1)\n",
    "print(l1_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a576565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
